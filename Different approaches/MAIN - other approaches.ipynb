{"metadata":{"colab":{"provenance":[],"collapsed_sections":["WwnLEzdENd7F","4QKlFXLlNd7P","K6YfG4QZNd7R","CF89ESfnNd7S","KHWrpoF3Nd7Y","BHjmQ-BMdfZP"]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8730254,"sourceType":"datasetVersion","datasetId":5240033},{"sourceId":8735056,"sourceType":"datasetVersion","datasetId":5243530},{"sourceId":8846715,"sourceType":"datasetVersion","datasetId":5324744},{"sourceId":8847231,"sourceType":"datasetVersion","datasetId":5325114}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5a43bf1d857343f6882881f8f167cac4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_211f47e8481b422e8362cb0cd5853931","IPY_MODEL_bf421bb45fee47e4a5b73264c1f0e22c","IPY_MODEL_3809d3c8ccaa490a8380c6a1fb938bab"],"layout":"IPY_MODEL_5edaa959ab5447cd8c829c6cc28d9e4f"}},"211f47e8481b422e8362cb0cd5853931":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c71c81cb0f1404e98016016d001997d","placeholder":"​","style":"IPY_MODEL_a6849504f03c4d7a9acbce6b6e7f25d1","value":"Fetching 5 files: 100%"}},"bf421bb45fee47e4a5b73264c1f0e22c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7cb7147fcb44f77b4c8ceb4eb0d9b2c","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_22ca6f25304e4c46ab5ecf69e11e8648","value":5}},"3809d3c8ccaa490a8380c6a1fb938bab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d59193c7c184c75a8e1c289c39bf178","placeholder":"​","style":"IPY_MODEL_ef2261616ea14302915cd54d7f8bfcb8","value":" 5/5 [00:00&lt;00:00, 237.09it/s]"}},"5edaa959ab5447cd8c829c6cc28d9e4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c71c81cb0f1404e98016016d001997d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6849504f03c4d7a9acbce6b6e7f25d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7cb7147fcb44f77b4c8ceb4eb0d9b2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22ca6f25304e4c46ab5ecf69e11e8648":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d59193c7c184c75a8e1c289c39bf178":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef2261616ea14302915cd54d7f8bfcb8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tools used","metadata":{}},{"cell_type":"markdown","source":"- Tuning hyperparameters\n  - Parsers = pdfplumer(removed header footer)\n  - Embedding model = BAAI/bge-large-en from HuggingFace\n  - Vectorstore = FAISS (fast), chromadb(slow)\n  - LLMs = llama3-70b-8192 from ollama(secure but slow), llama3-70b-8192 from groq(fast)\n  - Text splitters = RecursiveCharacterTextSplitter,ParentDocumentRetriever,  section-wise-chunking\n  - Two comparison approaches\n    - prompting to LLM after getting context from both pdfs\n    - prompting to LLM after getting both the answers (perfomrd better)\n\n  \n- a page in largest pdf contains 254 words on average\n","metadata":{"id":"SE1hGN3WNd7A"}},{"cell_type":"markdown","source":"# Installing Dependencies and libraries","metadata":{"id":"WwnLEzdENd7F"}},{"cell_type":"code","source":"import time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:11:41.541167Z","iopub.execute_input":"2024-08-05T10:11:41.541551Z","iopub.status.idle":"2024-08-05T10:11:41.546101Z","shell.execute_reply.started":"2024-08-05T10:11:41.541520Z","shell.execute_reply":"2024-08-05T10:11:41.545137Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"%%time\n# 3 min\n!pip install -q langchain\n# !pip install -q langchain-core\n!pip install -q langchain-community\n!pip install -q fastembed\n!pip install -q pypdf\n!pip install -q langchain_groq\n!pip install -q faiss-gpu\n!pip install -q sentence_transformers\n!pip install -q pdfplumber","metadata":{"id":"Vah0pcCjNd7F","outputId":"c4384e20-a866-49e0-bb7b-c03815bf8590","scrolled":true,"execution":{"iopub.status.busy":"2024-08-05T10:11:42.745748Z","iopub.execute_input":"2024-08-05T10:11:42.746970Z","iopub.status.idle":"2024-08-05T10:13:49.169706Z","shell.execute_reply.started":"2024-08-05T10:11:42.746927Z","shell.execute_reply":"2024-08-05T10:13:49.168576Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCPU times: user 1.57 s, sys: 349 ms, total: 1.92 s\nWall time: 2min 6s\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\nimport numpy as np \nimport pandas as pd\nimport random\nimport pdfplumber\nimport re\nfrom sklearn.metrics.pairwise import cosine_similarity\n# from langchain.embeddings import FastEmbedEmbeddings\nfrom langchain_community.embeddings import FastEmbedEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n# from langchain.chains.base import Chain\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.docstore.document import Document\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.prompts import PromptTemplate\nfrom langchain_groq import ChatGroq\nfrom langchain.chains import RetrievalQA\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom IPython.display import Markdown, display\n# from fastembed import LateInteractionTextEmbedding\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain.storage import InMemoryStore\nfrom tqdm.autonotebook import tqdm, trange","metadata":{"id":"PttkDKfSNd7G","execution":{"iopub.status.busy":"2024-08-05T10:13:49.172108Z","iopub.execute_input":"2024-08-05T10:13:49.172423Z","iopub.status.idle":"2024-08-05T10:13:51.240554Z","shell.execute_reply.started":"2024-08-05T10:13:49.172391Z","shell.execute_reply":"2024-08-05T10:13:51.239666Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Environment Variables","metadata":{"id":"uKrhDUh5Nd7H"}},{"cell_type":"code","source":"# AEBS PDFs\npath1 = \"/kaggle/input/pdffiles/GB AEBS.pdf\"\npath2 = \"/kaggle/input/pdffiles/UN AEBS.pdf\"\n\n# Light PDFs\npath3 = \"/kaggle/input/pdffiles/GB Lighting installation.pdf\"\npath4 = \"/kaggle/input/pdffiles/R048r12e.pdf\"\n\nqpath = \"/kaggle/input/queries-pdf/Questions_trial.pdf\"","metadata":{"id":"PmoHv-vSNd7J","outputId":"d62ac22f-fe33-459f-cf22-b75707b4aeef","execution":{"iopub.status.busy":"2024-08-05T10:13:51.241621Z","iopub.execute_input":"2024-08-05T10:13:51.242039Z","iopub.status.idle":"2024-08-05T10:13:51.246693Z","shell.execute_reply.started":"2024-08-05T10:13:51.242013Z","shell.execute_reply":"2024-08-05T10:13:51.245792Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"query_df = pd.read_csv(\"/kaggle/input/query-csv/Queries_46.csv\")[:46]\nquery_df.columns = [\"q_no\", \"query\", \"level\"]\nnew_queries = query_df['query'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:13:51.249405Z","iopub.execute_input":"2024-08-05T10:13:51.249695Z","iopub.status.idle":"2024-08-05T10:13:51.277585Z","shell.execute_reply.started":"2024-08-05T10:13:51.249672Z","shell.execute_reply":"2024-08-05T10:13:51.276877Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# LLMs\nllama3 = ChatGroq(groq_api_key = os.getenv('GROQ_API_KEY'), model = 'llama3-70b-8192', temperature=0.01)\n\n# For question answering\ntemplate1 = \"\"\"\nYou are the Vehicle Regulation Assistant, a helpful AI assistant.\nYour task is to answer given questions from the provided relevant part of the PDF.\nThe answer should be highly-detailed and well-sturctured.\nIf possible, refer to specific sections number within the context (e.g., \"According to section 4.1.2,...\").\nDo not begin your response with phrases like \"Based on the provided context, the answer to the question is:\".\nBe polite and helpful.\n\nCONTEXT: {context}\n\nQUESTION: {question}\n\"\"\"\ntemplate2 = \"\"\"\nYour task is to answer the question, using only the information provided in the given context.\nThe answer should be accuate and detailed.\nWhere applicable, refer to specific section numbers within the context (e.g., \"According to section 4.1.2,...\").\nIf the answer is not found in the provided context, simply state that there is no relevant information available \nwithout sharing details about the context.\n\nCONTEXT: {context}\n\nQUESTION: {question}\n\"\"\"\ntemplate3 = \"\"\"\nPlease provide a detailed and well-structured response to the question below, using only the information provided in the context.\nIf the context does not contain information related to the question, explicitly state that there is no relevant information in the provided context. \nBe polite and helpful.\nAlso, provide a confidence level from 0 to 100% in your response based on how certain you are about the information you have provided.\n\nCONTEXT: {context}\n\nQUESTION: {question}\n\"\"\"\n\ndspy_template = \"\"\"\n        System: You are a system specialized in legal document analysis. Your tasks are to analyze the provided pages of legal documents related to vehicle regulations. For each document, perform the following:\n            1. **Extract Relevant Content:** Identify and extract all relevant content related to the query from the document.\n            2. **Extract Numerical Values:** Extract all numerical values, as they are crucial for regulatory compliance.\n            3. **Summarize the Content:** Provide a detailed summary of the extracted information.\n            4. **Section References:** Refer to specific section numbers where applicable (e.g., \"According to section 4.1.2,...\").\n            5. **Missing Information:** Clearly state \"No relevant information available\" if the query cannot be answered based on the document.\n            6. **Confidence Level:** Return the confidence level of your response in percentage at the top of your response in **bold letters**.\n        If you receive two documents, follow these additional tasks:\n            1. **Group Similar Responses:** Group responses from both documents that cover similar aspects.\n            2. **Include Section Numbers:** Include section numbers from both documents where applicable.\n            3. **Comparison Table:** Create a table comparing the grouped responses. The table should include the following columns: Aspect, Document 1, Document 2, and Equivalence. Use these guidelines for evaluation:\n            - **Equivalent:** Information in both documents is essentially the same or conveys the same meaning.\n            - **Partially Equivalent:** Information is similar but contains differences in details or conditions.\n            - **Not Equivalent:** Information is significantly different or one document contains information not present in the other.\n            - **Notes:** Provide detailed notes explaining how the responses align or differ, including specific details or generalizations.\n            4. **Provide a JSON Summary:** Include a JSON summary of the comparison, indicating equivalence status and providing brief explanations.\n        Generate the comparison table and JSON summary as described.\n            **Example Table:**\n            | **Aspect**                        | **Document 1**                                                                                     | **Document 2**                                                                                                 | **Equivalence**      | **Notes**                                                                                                                   |\n            |-----------------------------------|--------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|----------------------|----------------------------------------------------------------------------------------------------------------------------|\n            | **Section Number**                | Section 6                                                                                       | Section 5                                                                                                      | Dis-equivalent       | The sections of the documents differ (6 vs. 5).                                                                                         |\n            | **Test Conditions**               | - Flat, dry concrete or asphalt surface with good adhesion                                      | - Test environment with horizontal visibility range                                                            | Partially Equivalent | Document 1 provides more specific details about test surface conditions, while Document 2 is more general. |\n            // other aspects\n            **json Summary:\n            [\n                \"equivalence_status\": \"Partially Equivalent\",\n                \"explanation\": \"The two documents have some similarities in their test procedures for lane departure warning systems, such as specifying test surface conditions and vehicle mass requirements. However, there are also notable differences in the specific tests and test parameters. Document 2 (GB/T 26773-2011) has additional tests for repeatability and false alarms, while Document 1 (UN/ECE Regulation No. 130) includes tests for failure detection and deactivation that are not present in Document 2. The warning test details and criteria also differ between the two documents.\"\n            ]\n    Context:\n    {context}\n    User: {question}\n    Bot:\"\"\"\nprompt = PromptTemplate(template=template2, input_variables=[\"question\", \"context\"])\n\ncombine_template = \"\"\"\nYour task is to answer the question, by synthesizing relevant information from the provided answers.\nThe answer should be accuate and detailed.\nWhere applicable, refer to specific section numbers within the context (e.g., \"According to section 4.1.2,...\").\nDo not reveal that the information comes from multiple answers, directly answer the question.\n\nQUESTION: {question}\n\nANSWER 1: {answer1}\n\nANSWER 2: {answer2}\n\"\"\"\n# For comparison RAG 1\ncomparison_template = \"\"\"\nWe have provided a question and their two answers. Generate a comparison section without a heading which includes whether both answer are same or partially same or different. If they are paritially same, then what is same and what is different. This comparison is based on the answers generated from both the contexts. Accuracy and precision are crucial for this task.\n\nQUESTION: {question}\n\nANSWER 1: {answer1}\n\nANSWER 2: {answer2}\n\"\"\"\n# For comparison RAG 2\ndef get_comparision_prompt(query, context1, context2):\n    comparison_template = \"\"\"\n    Response in three sections\n    \n    ANSWER 1: This is firts section, here answer the question form the context 1.\n    \n    ANSWER 2: This is second section, here answer the question form the context 2.\n    \n    COMPARISON: This is third section, here answer whether both answer are same or partially same or different. If they are paritially same, then what is same and what is different.\n    This section is completely based on answer generated in first and second section.\n    \n    Please answer the question solely based on the provided context. If you can't answer any of the both questions from their context then just tell that there is no answer in that context. This is very important for my life, be very precised and accurate in answering the question and also in comparison..\n\n    QUESTION: {question}\n\n    CONTEXT1: {context1}\n\n    CONTEXT2: {context2}\n    \"\"\"\n    comparison_prompt = comparison_template.format(context1 = context1,context2 = context2, question = query)\n    return comparison_prompt\n\n# Lightning\nold_queries = [\"Whats the difference between Grouped and Combined lamps?\", \"Can dipped-beam headlamp and main-beam headlamp for front lighting system?\", \"what is color of End Outline marker lamp?\", \"Can yellow lamp used as front fog lamp?\", \"Can red color light placed in the front of the vehicle?\", \"Can white light can be placed at the back of the vehicle?\", \"What are 1,1,a,1b,2a,2b,5,6 in direction indicator lamps?\", \"is cornering lamp mandatory?\", \"does reflective tape come under light and light signalling?\", \"standard weight of a person for testing?\",\"can dipped beam uses as a main beam?\", \"what are the light functions to be kept rear of the vehicle?\", \"What lamp should be fitted for passenger vehicles?\"]\nqueries = new_queries + old_queries","metadata":{"id":"PmoHv-vSNd7J","outputId":"d62ac22f-fe33-459f-cf22-b75707b4aeef","execution":{"iopub.status.busy":"2024-08-05T10:21:48.297431Z","iopub.execute_input":"2024-08-05T10:21:48.297834Z","iopub.status.idle":"2024-08-05T10:21:48.354752Z","shell.execute_reply.started":"2024-08-05T10:21:48.297803Z","shell.execute_reply":"2024-08-05T10:21:48.354040Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Embedding models","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-07-30T16:14:24.841212Z","iopub.execute_input":"2024-07-30T16:14:24.841540Z","iopub.status.idle":"2024-07-30T16:14:29.936046Z","shell.execute_reply.started":"2024-07-30T16:14:24.841514Z","shell.execute_reply":"2024-07-30T16:14:29.935268Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def get_embedding_model(model_name='stella_en_1.5B_v5'):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModel.from_pretrained(model_name)\n    return tokenizer, model\n\ndef embed_texts(texts, tokenizer, model):\n    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n    with torch.no_grad():\n        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n    return embeddings\n\ndef get_vectorstore1(path, embedding_model_name='stella_en_1.5B_v5'):\n    # Extract text from the document\n    text = extract_text(path)\n    \n    # Split the text into chunks\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n    texts = text_splitter.split_text(text)\n    docs = [Document(text) for text in texts if text.strip()]\n    \n    # Load the embedding model\n    tokenizer, model = get_embedding_model(embedding_model_name)\n    \n    # Embed the documents\n    embeddings = embed_texts([doc.page_content for doc in docs], tokenizer, model)\n    \n    # Create the FAISS vector store\n    vectorstore = FAISS.from_documents(docs, embeddings)\n    return vectorstore","metadata":{"execution":{"iopub.status.busy":"2024-08-05T06:05:42.384615Z","iopub.execute_input":"2024-08-05T06:05:42.385116Z","iopub.status.idle":"2024-08-05T06:05:42.394702Z","shell.execute_reply.started":"2024-08-05T06:05:42.385082Z","shell.execute_reply":"2024-08-05T06:05:42.393627Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Example usage\npath_to_document = path1\nvectorstore = get_vectorstore1(path_to_document)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nhf_bge_large = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-large-en\",\n                               model_kwargs={'device': 'cuda'},\n                               encode_kwargs={'normalize_embeddings': False})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embedding_model = FastEmbedEmbeddings(model_name = \"BAAI/bge-small-en-v1.5\")\n# fee_bge_large = FastEmbedEmbeddings(model_name = \"BAAI/bge-large-en-v1.5\")","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:14:19.154261Z","iopub.execute_input":"2024-08-05T10:14:19.154860Z","iopub.status.idle":"2024-08-05T10:14:19.159589Z","shell.execute_reply.started":"2024-08-05T10:14:19.154810Z","shell.execute_reply":"2024-08-05T10:14:19.158209Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"embedding_model = hf_bge_large\nembedding_model_header = hf_bge_large","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:14:19.160913Z","iopub.execute_input":"2024-08-05T10:14:19.161775Z","iopub.status.idle":"2024-08-05T10:14:19.191189Z","shell.execute_reply.started":"2024-08-05T10:14:19.161749Z","shell.execute_reply":"2024-08-05T10:14:19.190133Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Section wise chunking\n- after removing header footer","metadata":{}},{"cell_type":"code","source":"def embed_texts(texts):\n#     embedding = FastEmbedEmbeddings.embed_documents(embedding_model,texts = texts)\n    embedding = HuggingFaceBgeEmbeddings.embed_documents(embedding_model,texts = texts)\n    return embedding\n\ndef get_header_footer(pdf_path, threshold=0.71):\n    with pdfplumber.open(pdf_path) as pdf:\n        total_pages = len(pdf.pages)\n        if total_pages >= 15:\n            random_page_nos = random.sample(range(5, total_pages), 10)\n        else:\n            random_page_nos = list(range(total_pages))\n        \n        avg_similarity = 1\n        header_lines = -1\n        \n        while avg_similarity > threshold and header_lines < 4:\n            header_lines += 1\n            five_lines = []\n            \n            for page_no in random_page_nos:\n                lines = pdf.pages[page_no].extract_text().split('\\n')\n                if len(lines) > header_lines:\n                    five_lines.append(lines[header_lines])\n            similarities = cosine_similarity(embed_texts(five_lines))\n            avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n            \n        avg_similarity = 1\n        footer_lines = -1\n        \n        while avg_similarity > threshold and footer_lines < 4:\n            footer_lines += 1\n            five_lines = []\n            \n            for page_no in random_page_nos:\n                lines = pdf.pages[page_no].extract_text().split('\\n')\n                if len(lines) > footer_lines:\n                    five_lines.append(lines[-(footer_lines+1)])\n            similarities = cosine_similarity(embed_texts(five_lines))\n            avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n            \n        return header_lines, footer_lines\n    \ndef extract_text(pdf_path):\n    header_lines, footer_lines = get_header_footer(pdf_path)\n    with pdfplumber.open(pdf_path) as pdf:\n        text = ''\n        for page in pdf.pages:\n            page_text = page.extract_text()\n            if page_text:\n                lines = page_text.split('\\n')\n                if lines:\n                    page_text = '\\n'.join(lines[header_lines:-(footer_lines+1)])\n                    text += page_text + '\\n'\n        return text","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:14:19.193039Z","iopub.execute_input":"2024-08-05T10:14:19.193412Z","iopub.status.idle":"2024-08-05T10:14:19.264942Z","shell.execute_reply.started":"2024-08-05T10:14:19.193376Z","shell.execute_reply":"2024-08-05T10:14:19.263826Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"pattern = re.compile(r'\\n([1-9]|1[0-9])\\. [A-Z][a-zA-Z]+')   #United nations\n# pattern = re.compile('\\d\\. [A-Z]')                          # Smb United nations\n# pattern = re.compile(r'\\n([1-9]|1[0-9]) [A-Z][a-zA-Z]+')    #Chinese\n# pattern = re.compile(r'(\\n(1[0-9]|[1-9])\\s+[A-Z][a-zA-Z]+.*?)(?=\\n(?:[1-9]|1[0-5])\\s+[A-Z]|$)', re.DOTALL) #Chinese","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:14:19.268491Z","iopub.execute_input":"2024-08-05T10:14:19.268774Z","iopub.status.idle":"2024-08-05T10:14:19.359511Z","shell.execute_reply.started":"2024-08-05T10:14:19.268751Z","shell.execute_reply":"2024-08-05T10:14:19.358486Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def section_wise_chunking(pdf_path):\n    text = extract_text(pdf_path)\n    matches = list(pattern.finditer(text))\n    \n    # Use the positions of the matches to split the text into sections\n    sections = []\n    last_index = 0\n    for match in matches:\n        start, end = match.span()\n        section_text = text[last_index:start].strip()\n        if section_text:\n            sections.append(section_text)\n        last_index = start\n    if last_index < len(text):\n        sections.append(text[last_index:].strip())\n    \n    # Handeling too small and too large sections\n    text_chunks = []\n    previous_chunk_token_count = 0\n    for i, section in enumerate(sections):\n        tokens_count = len(section.split())\n        if i != 0 and tokens_count + last_chunk_token_count < 900:\n            text_chunks[-1] += \"\\n\"+section\n        elif tokens_count > 850:\n            splitted_chunks = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=300).split_text(section)\n            text_chunks += splitted_chunks[:1] + [splitted_chunks[0].split('\\n')[0]+' (Partial)\\n'+ chunk for chunk in splitted_chunks[1:]]\n        else:\n            text_chunks.append(section)\n        last_chunk_token_count = len(text_chunks[-1].split())\n    return text_chunks","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:14:19.361011Z","iopub.execute_input":"2024-08-05T10:14:19.361316Z","iopub.status.idle":"2024-08-05T10:14:19.447023Z","shell.execute_reply.started":"2024-08-05T10:14:19.361291Z","shell.execute_reply":"2024-08-05T10:14:19.445971Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# print(\"No. of chunks:\", len(chunks))\n# for i, chunk in enumerate(chunks):\n# #     print(chunk)\n#     print(\"\\n\" + \"_\"*0,\"chunk\",i, \"have\", len(chunk.split()), \"words\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:14:19.448411Z","iopub.execute_input":"2024-08-05T10:14:19.449283Z","iopub.status.idle":"2024-08-05T10:14:19.515587Z","shell.execute_reply.started":"2024-08-05T10:14:19.449254Z","shell.execute_reply":"2024-08-05T10:14:19.514585Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Get vectorstore\n- FAISS.from_documents takes list of docs as argument\n","metadata":{"id":"16NqA0VpNd7K"}},{"cell_type":"code","source":"# for RecursiveCharacterTextSplitter\ndef get_vectorstore1(path):\n    text = extract_text(path)\n    texts = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_text(text)\n    docs = [Document(text) for text in texts if text.strip()]\n#     docs = PyPDFLoader(path).load_and_split(RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True))\n    vectorstore = FAISS.from_documents(docs, embedding_model)\n    return vectorstore","metadata":{"id":"sbHLL94jNd7L","execution":{"iopub.status.busy":"2024-08-05T10:14:19.516819Z","iopub.execute_input":"2024-08-05T10:14:19.517146Z","iopub.status.idle":"2024-08-05T10:14:19.611093Z","shell.execute_reply.started":"2024-08-05T10:14:19.517121Z","shell.execute_reply":"2024-08-05T10:14:19.609791Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# for section_wise_chunking\ndef get_vectorstore2(path):\n    texts = section_wise_chunking(path)\n    docs = [Document(text) for text in texts if text.strip()]\n    vectorstore = FAISS.from_documents(docs, embedding_model)\n    return vectorstore","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:14:19.612330Z","iopub.execute_input":"2024-08-05T10:14:19.612637Z","iopub.status.idle":"2024-08-05T10:14:19.712235Z","shell.execute_reply.started":"2024-08-05T10:14:19.612610Z","shell.execute_reply":"2024-08-05T10:14:19.711141Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Double retrieval RAG","metadata":{}},{"cell_type":"code","source":"%%time\nretriever1 = get_vectorstore1(path4).as_retriever(search_kwargs={\"k\": 6})","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:14:19.713986Z","iopub.execute_input":"2024-08-05T10:14:19.714647Z","iopub.status.idle":"2024-08-05T10:15:07.391039Z","shell.execute_reply.started":"2024-08-05T10:14:19.714610Z","shell.execute_reply":"2024-08-05T10:15:07.390030Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"CPU times: user 47.2 s, sys: 297 ms, total: 47.5 s\nWall time: 47.6 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nretriever2 = get_vectorstore2(path4).as_retriever(search_kwargs={\"k\": 3})","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:15:07.392427Z","iopub.execute_input":"2024-08-05T10:15:07.392755Z","iopub.status.idle":"2024-08-05T10:15:40.049560Z","shell.execute_reply.started":"2024-08-05T10:15:07.392727Z","shell.execute_reply":"2024-08-05T10:15:40.048601Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"CPU times: user 32.7 s, sys: 161 ms, total: 32.9 s\nWall time: 32.7 s\n","output_type":"stream"}]},{"cell_type":"code","source":"def Double_RAG(query, verbose = 0):\n    \n    chain1 = RetrievalQA.from_llm(llm=llm, retriever=retriever1, prompt= prompt)\n    chain2 = RetrievalQA.from_llm(llm=llm, retriever=retriever2, prompt= prompt)\n    \n    answer1 = chain1.invoke(query)['result']\n    answer2 = chain2.invoke(query)['result']\n    \n    if verbose > 0:\n        print(\"ANSWER1:\", answer1)\n        print(\"ANSWER2:\", answer2)\n\n    combine_prompt = combine_template.format(question = query,answer1 = answer1, answer2 = answer2)\n    response = llm.invoke(combine_prompt).content\n    \n    return response","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:18:02.596752Z","iopub.execute_input":"2024-08-05T10:18:02.597369Z","iopub.status.idle":"2024-08-05T10:18:02.603885Z","shell.execute_reply.started":"2024-08-05T10:18:02.597340Z","shell.execute_reply":"2024-08-05T10:18:02.602902Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"query = queries[0]\nprint(f\"QUERY:{query} \")\ns = time.time()\nresponse = Double_RAG(query, 1) \ndisplay(Markdown(response))\nprint(\"_\"*60, \" generated in\", time.time()-s)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:21:56.760402Z","iopub.execute_input":"2024-08-05T10:21:56.760775Z","iopub.status.idle":"2024-08-05T10:21:58.089787Z","shell.execute_reply.started":"2024-08-05T10:21:56.760744Z","shell.execute_reply":"2024-08-05T10:21:58.088916Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"QUERY:What are the requirements for L category of vehicle? \nANSWER1: There is no relevant information available in the provided context regarding the requirements for L category of vehicle. The context only mentions categories M, N, O, and their subcategories, but does not mention category L.\nANSWER2: There is no relevant information available in the provided context regarding the requirements for L category of vehicles. The context only mentions categories M, N, O, and their subcategories, but does not mention category L.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Unfortunately, there is no relevant information available regarding the requirements for L category of vehicle. The provided context only mentions categories M, N, O, and their subcategories, but does not mention category L. Therefore, it is not possible to provide the requirements for L category of vehicle."},"metadata":{}},{"name":"stdout","text":"____________________________________________________________  generated in 1.3242483139038086\n","output_type":"stream"}]},{"cell_type":"code","source":"query = queries[10]\nprint(f\"QUERY:{query} \")\ns = time.time()\nresponse = Double_RAG(query,1) \ndisplay(Markdown(response))\nprint(\"_\"*60, \" generated in\", time.time()-s)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:22:19.094061Z","iopub.execute_input":"2024-08-05T10:22:19.094434Z","iopub.status.idle":"2024-08-05T10:22:56.272179Z","shell.execute_reply.started":"2024-08-05T10:22:19.094406Z","shell.execute_reply":"2024-08-05T10:22:56.271327Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"QUERY:What are the lights placed in front of the vehicle? \nANSWER1: According to the provided context, the lights placed in front of the vehicle are:\n\n* Front position lamp (2.7.14): used to indicate the presence and width of the vehicle when viewed from the front.\n* Daytime running lamp (2.7.25): used to make the vehicle more easily visible when driving during daytime.\n* Cornering lamp (2.7.26): used to provide supplementary illumination of that part of the road which is located near the forward corner of the vehicle.\n* Parking lamp (2.7.22): used to draw attention to the presence of a stationary vehicle in a built-up area, replacing the front and rear position lamps.\n* End-outline marker lamp (2.7.23): fitted near to the extreme outer edge and as close as possible to the top of the vehicle, intended to indicate clearly the vehicle's overall width.\n* Main-beam headlamps (6.1.7.3 and 6.1.7.4): used for illumination of the road ahead.\n\nNote that these lights may not be exhaustive, as the context provides a wide range of lighting devices, and some may not be specifically mentioned as being placed in front of the vehicle.\nANSWER2: According to the provided context, the lights placed in front of the vehicle are:\n\n* Headlamps (2.7.9.)\n* Daytime running lamps (2.7.25.)\n* Cornering lamps (2.7.26.)\n* Adaptive front lighting system (AFS) (2.7.28.)\n* Front position lamps (2.7.10.)\n* Parking lamps (2.7.22.) (used to draw attention to the presence of a stationary vehicle in a built-up area)\n\nNote that these lights may have different functions and characteristics, but they are all placed in front of the vehicle.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"The lights placed in front of the vehicle are:\n\n* Front position lamp: used to indicate the presence and width of the vehicle when viewed from the front.\n* Daytime running lamp: used to make the vehicle more easily visible when driving during daytime.\n* Cornering lamp: used to provide supplementary illumination of that part of the road which is located near the forward corner of the vehicle.\n* Parking lamp: used to draw attention to the presence of a stationary vehicle in a built-up area, replacing the front and rear position lamps.\n* End-outline marker lamp: fitted near to the extreme outer edge and as close as possible to the top of the vehicle, intended to indicate clearly the vehicle's overall width.\n* Main-beam headlamps: used for illumination of the road ahead.\n* Headlamps: used for illumination of the road ahead.\n* Adaptive front lighting system (AFS): used to provide adaptive illumination of the road ahead.\n\nThese lights are placed in front of the vehicle and serve various purposes, including indicating the vehicle's presence, providing illumination, and enhancing safety."},"metadata":{}},{"name":"stdout","text":"____________________________________________________________  generated in 37.172677755355835\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"List all the lamps and lights mentioned in the pdf. Also mention which of them are placed in front or rear of the vehicle.\"\nprint(f\"QUERY:{query} \")\ns = time.time()\nresponse = Double_RAG(query, 1) \ndisplay(Markdown(response))\nprint(\"_\"*60, \" generated in\", time.time()-s)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:25:21.586953Z","iopub.execute_input":"2024-08-05T10:25:21.587403Z","iopub.status.idle":"2024-08-05T10:25:32.510196Z","shell.execute_reply.started":"2024-08-05T10:25:21.587369Z","shell.execute_reply":"2024-08-05T10:25:32.509252Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"QUERY:List all the lamps and lights mentioned in the pdf. Also mention which of them are placed in front or rear of the vehicle. \nANSWER1: Based on the provided context, the following lamps and lights are mentioned:\n\n1. Front position lamps (placed in the front of the vehicle)\n2. Rear position lamps (placed in the rear of the vehicle)\n3. End-outline marker lamps (placed near the extreme outer edge and as close as possible to the top of the vehicle, can be placed in the front or rear)\n4. Side marker lamps (placed on the side of the vehicle)\n5. Rear registration plate lamp (placed in the rear of the vehicle)\n6. Parking lamps (can replace front and rear position lamps when the vehicle is stationary in a built-up area)\n7. Daytime running lamps (facing in a forward direction, placed in the front of the vehicle)\n8. Cornering lamps (used to provide supplementary illumination of the road near the forward corner of the vehicle, placed in the front of the vehicle)\n9. Adaptive front lighting system (AFS) (placed in the front of the vehicle)\n10. Front retro-reflectors (non-triangular, placed in the front of the vehicle)\n11. Side retro-reflectors (non-triangular, placed on the side of the vehicle)\n12. Rear retro-reflectors (non-triangular, placed in the rear of the vehicle)\n13. Direction-indicator lamps (categories 1, 1a, 1b, 2a, 2b, 5, and 6, can be placed in the front, rear, or side of the vehicle)\n14. Manoeuvring lamps (placed in the front or rear of the vehicle)\n15. Emergency stop signal (placed in the rear of the vehicle)\n16. Exterior courtesy lamp (placed in the front or rear of the vehicle, provides courtesy lighting)\n\nNote that some lamps may be mentioned multiple times in the context, but they are listed here only once.\nANSWER2: Based on the provided context, the following lamps and lights are mentioned:\n\n1. Front position lamps (front)\n2. Rear position lamps (rear)\n3. Parking lamps (front and rear)\n4. End-outline marker lamps (front and rear)\n5. Side marker lamps (side)\n6. Daytime running lamps (front)\n7. Cornering lamps (front)\n8. Rear fog lamp (rear)\n9. Stop lamp (rear)\n10. Direction-indicator lamps (front and rear)\n11. Rear retro-reflector, non-triangular (rear)\n12. Conspicuity markings (various locations)\n13. Manoeuvring lamp (side)\n14. Exterior courtesy lamp (various locations)\n15. Adaptive front lighting system (AFS) (front)\n16. Lighting unit (part of AFS, front)\n17. Installation unit (part of AFS, front)\n18. Interdependent lamp system (various locations)\n19. Interdependent lamp (part of interdependent lamp system, various locations)\n\nNote that some of these lamps and lights may be mentioned multiple times in the context, but they are listed here only once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Here is the comprehensive list of lamps and lights mentioned in the context, along with their placement on the vehicle:\n\n1. Front position lamps (front)\n2. Rear position lamps (rear)\n3. End-outline marker lamps (front and rear)\n4. Side marker lamps (side)\n5. Rear registration plate lamp (rear)\n6. Parking lamps (front and rear)\n7. Daytime running lamps (front)\n8. Cornering lamps (front)\n9. Adaptive front lighting system (AFS) (front)\n10. Front retro-reflectors (non-triangular, front)\n11. Side retro-reflectors (non-triangular, side)\n12. Rear retro-reflectors (non-triangular, rear)\n13. Direction-indicator lamps (categories 1, 1a, 1b, 2a, 2b, 5, and 6, front, rear, or side)\n14. Manoeuvring lamps (front or rear)\n15. Rear fog lamp (rear)\n16. Stop lamp (rear)\n17. Emergency stop signal (rear)\n18. Exterior courtesy lamp (front or rear)\n19. Lighting unit (part of AFS, front)\n20. Installation unit (part of AFS, front)\n21. Interdependent lamp system (various locations)\n22. Interdependent lamp (part of interdependent lamp system, various locations)\n23. Conspicuity markings (various locations)\n\nNote that some lamps may have multiple categories or types, but they are listed here only once."},"metadata":{}},{"name":"stdout","text":"____________________________________________________________  generated in 10.916836977005005\n","output_type":"stream"}]},{"cell_type":"code","source":"saved_responses = []\nfor q_no, query in enumerate(queries):\n    print(f\"QUERY {q_no}:{query} \")\n    s = time.time()\n    response = Double_RAG(query) \n    print(\"RESPONSE:\")\n    display(Markdown(response))\n    print(\"_\"*60, \" generated in\", time.time()-s)\n    saved_responses.append({\"query\": query, \"response\": response})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(saved_responses)\ndf.to_csv('responses_DRAG_.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_vectorstore3(path):\n    text = extract_text(path)\n    texts = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_text(text)\n    colbert1.index(collection=texts,index_name=\"file1\",max_document_length=10000,split_documents=True)\n    vectorstore = colbert1.as_langchain_retriever(k=6)\n    return vectorstore\n\ndef get_vectorstore4(path):\n    texts = section_wise_chunking(path)\n    colbert2.index(collection=texts,index_name=\"file2\",max_document_length=10000,split_documents=True)\n    vectorstore = colbert2.as_langchain_retriever(k=3)\n    return vectorstore","metadata":{"execution":{"iopub.status.busy":"2024-07-30T15:46:45.973932Z","iopub.execute_input":"2024-07-30T15:46:45.974585Z","iopub.status.idle":"2024-07-30T15:46:45.981111Z","shell.execute_reply.started":"2024-07-30T15:46:45.974552Z","shell.execute_reply":"2024-07-30T15:46:45.980020Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"%%time\nretriever3 = get_vectorstore3(path4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nretriever4 = get_vectorstore4(path4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Double_RAG(query, verbose = 0):\n    \n    chain1 = RetrievalQA.from_llm(llm=llm, retriever=retriever1, prompt= prompt)\n    chain2 = RetrievalQA.from_llm(llm=llm, retriever=retriever2, prompt= prompt)\n    \n    answer1 = chain1.invoke(query)['result']\n    answer2 = chain2.invoke(query)['result']\n    \n    if verbose > 0:\n        print(\"ANSWER1:\", answer1)\n        print(\"ANSWER2:\", answer2)\n\n    combine_prompt = combine_template.format(question = query,answer1 = answer1, answer2 = answer2)\n    response = llm.invoke(combine_prompt).content\n    \n    return response","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def RAG1(query):\n    qa_chain = RetrievalQA.from_llm(llm=llama3, retriever=RAG.as_langchain_retriever(k=3), prompt= prompt)\n    return qa_chain.invoke(query)['result']","metadata":{"execution":{"iopub.status.busy":"2024-07-30T15:25:01.770377Z","iopub.execute_input":"2024-07-30T15:25:01.770786Z","iopub.status.idle":"2024-07-30T15:25:01.776200Z","shell.execute_reply.started":"2024-07-30T15:25:01.770757Z","shell.execute_reply":"2024-07-30T15:25:01.775142Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"RAG1(queries[10])","metadata":{"execution":{"iopub.status.busy":"2024-07-30T15:25:10.616830Z","iopub.execute_input":"2024-07-30T15:25:10.617210Z","iopub.status.idle":"2024-07-30T15:25:11.476949Z","shell.execute_reply.started":"2024-07-30T15:25:10.617180Z","shell.execute_reply":"2024-07-30T15:25:11.476037Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'According to section 6.22.4.2, all lighting units of an AFS (Adaptive Front-lighting System) shall be mounted at the front of the vehicle. Additionally, section 6.12.1 mentions that parking lamps are optional on motor vehicles not exceeding 6 m in length and not exceeding 2 m in width. Therefore, the lights placed in front of the vehicle are the AFS lighting units and possibly parking lamps, if installed.'"},"metadata":{}}]},{"cell_type":"code","source":"from langchain.chains import create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_core.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_template(\n    \"\"\"Answer the following question based only on the provided context:\n\n<context>\n{context}\n</context>\n\nQuestion: {input}\"\"\"\n)\n\n# creating Document stuff chain\ndocument_chain = create_stuff_documents_chain(llm, prompt)\n\n# creating retrival chain\nretrieval_chain = create_retrieval_chain(retriever, document_chain)\n\n\n# invoking the chain \nretrieval_chain.invoke({\"input\": queries[10]})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/stanford-futuredata/ColBERT.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RAG 1\n- Using FAISS retriever\n- Adv. any k\n- RetrievalQA chain","metadata":{}},{"cell_type":"code","source":"%%time\n# retriever = get_vectorstore_2(path4).as_retriever(search_kwargs={\"k\": 4})\nretriever = vectorstore_path4.as_retriever()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def RAG1(query):\n    qa_chain = RetrievalQA.from_llm(llm=llama3, retriever=retriever, prompt= prompt)\n    return qa_chain.invoke(query)['result']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\nfor query in queries4:\n    print(\"Query:\",query)\n    print(\"Tokens:\",[len(retriever.invoke(query)[i].page_content.split()) for i in range(4)])   \n    response = RAG1(query) \n    display(Markdown(response))\n    print(\"_\"*100)\n    data.append({\"query\": query, \"response\": response})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(data)\ndf.to_excel('responses10_prompt3.xlsx', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q = \"what is color of End Outline marker lamp?\"\nfor context in retriever.invoke(q):\n    print(context.page_content)\n    print(\"_\"*80)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison_RAG2\n- using PDR\n- Fixed k = 4","metadata":{}},{"cell_type":"code","source":"def PDR(path):\n    documents = PyPDFLoader(path).load()\n    combined_text = \"\\n\".join(document.page_content for document in documents)\n    document = [Document(page_content=combined_text, metadata={\"source\": path})]\n\n    retriever = ParentDocumentRetriever(vectorstore=Chroma(collection_name=\"full_documents\", embedding_function=FastEmbedEmbeddings()),\n                                        docstore=InMemoryStore(),\n                                        child_splitter=RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100),\n                                        parent_splitter=RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100))\n    \n    retriever.add_documents(document, ids=None)\n    return retriever","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nretriever2 = PDR(path4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Comparison_RAG2(query):\n    qa_chain = RetrievalQA.from_llm(llm=llama3, retriever=retriever2, prompt= prompt)\n    return qa_chain.invoke(query)['result']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\nfor query in queries4:\n    print(\"#\",query)\n    response = Comparison_RAG2(query) \n    print(response)\n    print(\"---------------------------------------------------------------------\")\n    data.append({\"query\": query, \"response\": response})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save responses and export\ndf = pd.DataFrame(data)\ndf.to_excel('query_responses10.xlsx', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison_RAG 1\n- RAG for comparison\n- by FAISS Retrieval\n- by calling LLM 3 times\n- chain - RetrievalQA","metadata":{}},{"cell_type":"code","source":"%%time\nretriever1 = get_vectorstore(path1).as_retriever(search_kwargs={\"k\": 6})\nretriever2 = get_vectorstore(path2).as_retriever(search_kwargs={\"k\": 6})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Comparison_RAG1(query):\n    llm = llama3\n    \n    qa_chain1 = RetrievalQA.from_llm(llm=llm, retriever=retriever1, prompt= prompt,)\n    qa_chain2 = RetrievalQA.from_llm(llm=llm, retriever=retriever2, prompt= prompt)\n\n    answer1 = qa_chain1.invoke(query)['result']\n    answer2 = qa_chain2.invoke(query)['result']\n    \n    comparison_prompt = comparison_template.format(question = query,answer1 = answer1, answer2 = answer2)\n    comparison = llm.invoke(comparison_prompt).content\n    \n    response = f\"**ANSWER 1**: {answer1}\\n\\n**ANSWER 2**: {answer2}\\n\\n**COMPARISION**: {comparison}\"\n    return response","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AEBS\nqueries2 = [\"Explain the test procedures in detail, in details.\", \"What are warning indications, in details.\"]\nfor query in queries2:\n    display(Markdown(Co1parison_RAG1(query)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison_RAG 2\n- by PDR\n- 3 LLM calls","metadata":{"execution":{"iopub.status.busy":"2024-06-20T17:15:16.093327Z","iopub.execute_input":"2024-06-20T17:15:16.093707Z","iopub.status.idle":"2024-06-20T17:15:16.099623Z","shell.execute_reply.started":"2024-06-20T17:15:16.093678Z","shell.execute_reply":"2024-06-20T17:15:16.098532Z"}}},{"cell_type":"code","source":"retriever1 = PDR(path1)\nretriever2 = PDR(path2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Comparison_RAG2(query):\n    context1 = get_context(query, retriever1)\n    context2 = get_context(query, retriever2)\n\n    qa_prompt1 = qa_template.format(question = query,context = context1)\n    qa_prompt2 = qa_template.format(question = query,context = context2)\n    \n    answer1 = llama3.invoke(qa_prompt1).content\n    answer2 = llama3.invoke(qa_prompt2).content\n    \n    comparison_prompt = comparison_template.format(question = query,answer1 = answer1, answer2 = answer2)\n    comparison = llm.invoke(comparison_prompt).content\n    \n    response = f\"**ANSWER 1**: {answer1}\\n\\n**ANSWER 2**: {answer2}\\n\\n**COMPARISION**: {comparison}\"\n    return response","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison_RAG 3\n- Vectostore to context from scratch`\n- Dis. - 1 prompt, 1 LLM call, too much load on geneartion","metadata":{}},{"cell_type":"code","source":"def get_context(query, vectorstore):\n    retrieved_docs = vectorstore.similarity_search_with_relevance_scores(query, k = 6)\n    context = \"\"\n    for doc in retrieved_docs:\n        context += doc.page_content\n    return context","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorstore1 = get_vectorstore(path1)\nvectorstore2 = get_vectorstore(path2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Comparison_RAG3(query):\n    context1 = get_context(query, retriever1)\n    context2 = get_context(query, retriever2)\n    \n    prompt = get_comparision_prompt(query, context1, context2)\n    \n    return llama3.invoke(prompt).content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#AEBS\nqueries1 = [\"Explain the test procedures in detail.\",\"When collision early warning signal shall be sent?\",\"What are warning indications, in details.\",\"What AEBS should do in vehicle ignition?\",\"The total speed reduction of the subject vehicle at the time of the collision with the stationary target shall be not less than how many kilometers per hour?\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for query in queries1:\n    print(\"#\",query)\n    display(Markdown(Comparison_RAG3(query)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Some notes","metadata":{}},{"cell_type":"code","source":"#multivector embeddings have best performance for retrieval https://www.rungalileo.io/blog/mastering-rag-how-to-select-an-embedding-model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optional way for RAG2# def get_context(query, retriever):\n#     retrieved_docs = retriever.get_relevant_documents(query)\n#     context = \" \".join([doc.page_content for doc in retrieved_docs])\n#     return context\n\n# def RAG2(query):\n#     context = get_context(query,retriever1)\n#     qa_prompt = template2.format(question = query,context = context)\n#     return llama3.invoke(qa_prompt).content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Documentation","metadata":{}},{"cell_type":"markdown","source":"- llama3 could be taken from groq or ollama\n- Possible raesons for latency in pdr-llama2-chromadb code\n  - llama2 downloaded in local computer\n  - slow parent document retriever\n  - small cpu\n- Giskard for evaluation\n  - By default uses gpt4 but llama3 also could be connected\n  - it finds contexts from pdf/url and generates question, further context is matched to the RAG response for evaluating RAG.\n- Use an image processing model (e.g., CLIP, a Vision Transformer, or a CNN) to convert images into embeddings or textual descriptions.","metadata":{}},{"cell_type":"markdown","source":"# Optional codes","metadata":{}},{"cell_type":"markdown","source":"### Section-wise chunking (ss)","metadata":{}},{"cell_type":"code","source":"# Initialize Anthropic client\nclient = Anthropic()\n\ndef count_tokens(text):\n    '''\n    Count the number of tokens in the provided text using Anthropic's client.\n\n    Args:\n    - text (str): The text for which tokens need to be counted.\n\n    Returns:\n    - int: Number of tokens in the text.\n    '''\n    return client.count_tokens(text)\n\ndef extract_all_text(pdf_path):\n    '''\n    Extract all text from a PDF document.\n\n    Args:\n    - pdf_path (str): Path to the PDF file.\n\n    Returns:\n    - dict: Dictionary where keys are page numbers and values are extracted text strings.\n    '''\n    with pdfplumber.open(pdf_path) as pdf:\n        text_dict = {}\n        for i, page in enumerate(pdf.pages, start=1):\n            if i == 1:\n                text_dict[i] = page.extract_text(layout=False, strip=True, return_chars=True)\n                \n            else:\n                text_dict[i] = '\\n'.join(line for line in page.extract_text().split('\\n') if 'Official Journal of the European Union' not in line)\n    return text_dict\n\n# Function to concatenate lines where the element before the period (.) is the same\ndef concat_lines_by_same_element(text):\n    '''\n    Concatenate lines where the element before the period (.) is the same into blocks.\n\n    Args:\n    - text (str): Text to process.\n\n    Returns:\n    - list: List of concatenated blocks.\n    '''\n    lines = text.split('\\n')\n    concatenated_blocks = []\n    current_block = \"\"\n    current_element = None\n    \n    for line in lines:\n        match = re.match(r'^(\\d+)\\.', line.strip())\n        if match:\n            element = match.group(1)\n            if current_element is None:\n                if current_block:\n                    concatenated_blocks.append(current_block.strip())\n                current_element = element\n                current_block = line\n            elif element == current_element:\n                current_block += \" \" + line\n            else:\n                concatenated_blocks.append(current_block.strip())\n                current_element = element\n                current_block = line\n        else:\n            current_block += \" \" + line\n    \n    if current_block:\n        concatenated_blocks.append(current_block.strip())\n    \n    return concatenated_blocks\n\ndef concatenate_blocks(blocks, max_tokens=2000):\n    '''\n    Concatenate text blocks ensuring the total token count for each concatenated block does not exceed the specified limit.\n\n    Args:\n    - blocks (list): List of text blocks to concatenate.\n    - max_tokens (int): Maximum number of tokens allowed per concatenated block (default is 2000).\n\n    Returns:\n    - list: List of concatenated blocks where each block's total token count is within the specified limit.\n    '''\n    concatenated_blocks = []\n    temp_block = []\n    total_value = 0\n    \n    for ele in blocks:\n        value = count_tokens(ele)\n        if total_value + value <= max_tokens:\n            temp_block.append(ele)\n            total_value += value\n        else:\n            print(f\"Total tokens for current block: {total_value}\")\n            concatenated_blocks.append(' '.join(temp_block))\n            temp_block = [ele]\n            total_value = value\n    \n    # Add the last block if it's not empty\n    if temp_block:\n        concatenated_blocks.append(' '.join(temp_block))\n    \n    return concatenated_blocks\n\n# %%\n# Example usage of extract_all_text function\npdf_path = path1\nall_text = extract_all_text(pdf_path)\n\n# %%\nfull_text = '\\n'.join(all_text.values())\n# Encode some misc unicode characters\nfull_text = full_text.encode('utf-8').decode()\n# Example usage of concat_lines_by_same_element function\nblocks = concat_lines_by_same_element(full_text)  # Assuming all_text is a dictionary with page numbers\nprint(blocks)\nblocks_1 = concatenate_blocks(blocks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Section-wise chunking (smb)","metadata":{}},{"cell_type":"code","source":"import pdfplumber\nimport re\n\ndef read_pdf_pagewise(file_path):\n    \n    section_pattern = re.compile(r'(\\d [A-Z]|Annex [A-Z])')\n\n    sections = {}\n    with pdfplumber.open(file_path) as pdf:\n        num_pages = len(pdf.pages)\n        print(f'Total pages: {num_pages}')\n\n        current_section = None\n        for page_num in range(2, num_pages):\n            page = pdf.pages[page_num]\n            page_text = page.extract_text()\n\n            if page_text:\n                lines = page_text.split('\\n')\n                if len(lines) > 3:  \n                    lines = lines[1:-2]\n                else:\n                    lines = [] \n                \n                for line in lines:\n                    match = section_pattern.match(line)\n                    if match:\n                        current_section = match.group(1)\n                        if current_section not in sections:\n                            sections[current_section] = []\n                    \n                    if current_section:\n                        sections[current_section].append(line)\n    return sections\n#     for section, content in sections.items():\n#         print(f\"--- {section} ---\")\n#         print('\\n'.join(content))\n#         print(\"\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}