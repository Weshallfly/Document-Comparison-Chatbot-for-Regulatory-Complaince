# Document Comparison Chatbot for Regulatory Compliance

## Introduction
The 'Document Comparison Chatbot for Regulatory Compliance' project focuses on automating the comparison of regulatory documents across different countries. This is particularly crucial for companies in the automotive industry who need to comply with varying regulations when entering new markets. The chatbot accepts two regulatory documents and a user query, then generates answers and provides a comparative analysis, highlighting similarities and differences.

## Project Overview

### Background and Motivation
- Each country has its own rules and regulations related to vehicles. These rules and regulations must be followed for a legal sale of vehicles.
- Difficulty in comparing documents manually from different countries with varying regulations.

### **Objective**
Develop a chatbot to compare legal documents from different countries based on user queries and generate accurate responses to support regulatory compliance.

### **Sub-Objectives**
To generate precise and contextually accurate responses to user queries.

## The Why
The main motivations for this project include reducing the time and cost associated with manual document comparison, ensuring accuracy in adherence to regulations, and overcoming language barriers. Manually comparing regulatory documents can take up to 60 hours per set of documents. Automating this process not only saves time but also improves accuracy by reducing human error.

The homologation department, responsible for ensuring products meet international regulations, faces a significant challenge in efficiently comparing regulatory documents across various languages. This project proposes developing a specialized chatbot for the homologation department with a scope for automated document comparison using LLM and translation of various languages into a single source language.

## Approaches and Challenges

### **1. Retrieval-Augmented Generation (RAG)**
- **Approach**: Initially, we implemented RAG to generate answers from specific domains or knowledge bases. This involved retrieving relevant text segments and then generating answers based on these segments.
- **Results**: The approach provided some accurate responses but struggled with incomplete answers and missing context.
- **Challenges**: The complex structure of the PDF documents with multiple nested sections and varying formats posed a challenge for the RAG model, making it difficult to maintain context and accuracy.

### **2. Page-wise Cumulative Summarization**
- **Approach**: To tackle the context maintenance issue, we experimented with cumulative page-wise summarization. Each page summary was generated by considering the current page and the summary of the previous pages.
- **Results**: This method improved context continuity but was highly time-consuming, requiring one LLM call per page.

### **3. RAG with Parent Document Retriever**
- **Approach**: We employed a parent document retriever approach that utilized smaller chunks for information retrieval and larger chunks for generating answers, balancing the strengths of both chunk sizes.
- **Results**: This technique improved retrieval and generation quality but occasionally broke sections, leading to fragmented context.

### **4. Section-wise Chunking**
- **Approach**: To address the issue of fragmented context, we switched to section-wise chunking, treating each section as an independent chunk.
- **Results**: This method preserved section integrity but struggled with very large sections (e.g., 50-page sections), particularly when concise answers were required.

### **5. Custom Multi-Retriever RAG**
- **Approach**: A custom multi-retriever RAG was developed, using two retrievers for handling smaller and larger chunks.
- **Results**: Achieved high accuracy by leveraging the advantages of both retriever types, optimizing the retrieval process.

### **6. DSPy for Optimized Prompts**
- **Approach**: Leveraged DSPy to fine-tune and optimize prompts using carefully selected examples. This involved adjusting prompt structures and content to better guide the LLM's output based on specific retrieval contexts.
- **Results**: The optimized prompts led to accurate and contextually relevant responses, enhancing the overall generation process.

## Major Challenges
Here are some major challenges we encountered while experimenting with different approaches:

### **1. Complex PDF Structure**
The complex structure of the PDFs, with 6-7 levels of nested sections, posed a significant challenge. Understanding the context of a particular section often depended on its ancestor sections, making it difficult to maintain coherence and relevance throughout the extraction process.

### **2. Optimal Chunk Size**
Determining the appropriate chunk size for different types of questions was problematic. Some questions required concise, one-line answers, while others necessitated detailed responses. Striking the right balance between chunk size and answer detail proved challenging.

### **3. Dispersed Information**
Answers were often spread across various parts of the PDF. This dispersion required the retriever to extract and compile information from multiple sections, complicating the retrieval process.

## Evaluation
We have done evaluation manually. Following are the metrics and how we have evaluated them manually.

- **High Accuracy**: Ensured all facts in the answer are correct.
- **High Recall**: Answers are exhaustive, covering all relevant details.
- **High Precision**: No irrelevant facts are included; all information is directly related to the query.

## Results
Most results were accurate and complete, but there's always room for improvement. Further enhancements could be made by refining the DSPy implementation.

## Application
Access the Application: [Document Comparison Chatbot](https://document-comparison-chatbot.streamlit.app/)

### Reason for **High Latency** in Response Generation:
- **Complex Workflow**: The process involved 4 vector stores and 7 LLM calls, contributing to delays.
- **Potential Solutions**: This application is deployed for our personal showcase. Clients could switch to cloud services like Claude from Bedrock, which could significantly reduce response times.

## Future Improvements
Here are some potential future improvements that could be explored further:
- Future work could enhance multi-language support and develop a robust knowledge database for better handling of diverse regulatory documents.
- Optimizing computational efficiency and response times using advanced techniques and cloud services like Claude from Bedrock is another area for improvement.
- Due to time constraints, we implemented a minimal version of DSPy. However, there is potential for further refinement and enhancement in future iterations.

## Conclusion
The Document Comparison Chatbot project has made significant strides in automating the compliance verification process. Within the limited time available, we tested and refined various approaches, achieving notable improvements. However, there is always room for further enhancement. Future work could focus on improving efficiency and expanding capabilities to make the tool even more versatile and effective across diverse regulatory scenarios.

## Libraries and Tools
- **Libraries**: langchain, pypdf, langchain_groq, sentence_transformers, pdfplumber, faiss-cpu, streamlit
- **Tools**: FAISS, RecursiveCharacterTextSplitter, HuggingFace BGE Embeddings, PromptTemplate, DSPy, RetrievalQA
