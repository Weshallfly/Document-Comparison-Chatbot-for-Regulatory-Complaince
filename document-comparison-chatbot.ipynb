{"metadata":{"colab":{"provenance":[],"collapsed_sections":["WwnLEzdENd7F","4QKlFXLlNd7P","K6YfG4QZNd7R","CF89ESfnNd7S","KHWrpoF3Nd7Y","BHjmQ-BMdfZP"]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8730254,"sourceType":"datasetVersion","datasetId":5240033},{"sourceId":8735056,"sourceType":"datasetVersion","datasetId":5243530}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5a43bf1d857343f6882881f8f167cac4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_211f47e8481b422e8362cb0cd5853931","IPY_MODEL_bf421bb45fee47e4a5b73264c1f0e22c","IPY_MODEL_3809d3c8ccaa490a8380c6a1fb938bab"],"layout":"IPY_MODEL_5edaa959ab5447cd8c829c6cc28d9e4f"}},"211f47e8481b422e8362cb0cd5853931":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c71c81cb0f1404e98016016d001997d","placeholder":"​","style":"IPY_MODEL_a6849504f03c4d7a9acbce6b6e7f25d1","value":"Fetching 5 files: 100%"}},"bf421bb45fee47e4a5b73264c1f0e22c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7cb7147fcb44f77b4c8ceb4eb0d9b2c","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_22ca6f25304e4c46ab5ecf69e11e8648","value":5}},"3809d3c8ccaa490a8380c6a1fb938bab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d59193c7c184c75a8e1c289c39bf178","placeholder":"​","style":"IPY_MODEL_ef2261616ea14302915cd54d7f8bfcb8","value":" 5/5 [00:00&lt;00:00, 237.09it/s]"}},"5edaa959ab5447cd8c829c6cc28d9e4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c71c81cb0f1404e98016016d001997d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6849504f03c4d7a9acbce6b6e7f25d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7cb7147fcb44f77b4c8ceb4eb0d9b2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22ca6f25304e4c46ab5ecf69e11e8648":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d59193c7c184c75a8e1c289c39bf178":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef2261616ea14302915cd54d7f8bfcb8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- Tuning hyperparameters\n  - Parsers = pdfplumer(removed header footer)\n  - Embedding model = FastEmbedEmbeddings(BAAI/bge-small-en-v1.5)\n  - Vectorstore = FAISS(fast), chromaDB(slow)\n  - LLMs = llama3-70b-8192(robust), mixtral-8x7b-32768 (speciafic task)\n  - Other hyperparameters = k, chunk_size\n  - RAG Implemetations = (1) using RCTSplitter, (2) using section-wise-chunking\n\n- Improvements\n  - better embedding model\n  - make it conversational\n  - reranking, query transformation techniques","metadata":{"id":"SE1hGN3WNd7A"}},{"cell_type":"markdown","source":"# Installing Dependencies and libraries","metadata":{"id":"WwnLEzdENd7F"}},{"cell_type":"code","source":"import time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:32:33.267417Z","iopub.execute_input":"2024-06-30T07:32:33.267751Z","iopub.status.idle":"2024-06-30T07:32:33.278375Z","shell.execute_reply.started":"2024-06-30T07:32:33.267725Z","shell.execute_reply":"2024-06-30T07:32:33.277472Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%%time\n# 3 min\n!pip install -q langchain\n!pip install -q langchain-core\n!pip install -q langchain-community\n!pip install -q fastembed\n!pip install -q pypdf\n!pip install -q langchain_groq\n!pip install -q faiss-gpu\n!pip install -q sentence_transformers\n!pip install -q pdfplumber","metadata":{"id":"Vah0pcCjNd7F","outputId":"c4384e20-a866-49e0-bb7b-c03815bf8590","scrolled":true,"execution":{"iopub.status.busy":"2024-06-30T07:32:33.279967Z","iopub.execute_input":"2024-06-30T07:32:33.280285Z","iopub.status.idle":"2024-06-30T07:34:49.961738Z","shell.execute_reply.started":"2024-06-30T07:32:33.280254Z","shell.execute_reply":"2024-06-30T07:34:49.960572Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCPU times: user 1.45 s, sys: 314 ms, total: 1.76 s\nWall time: 2min 16s\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\nimport numpy as np \nimport pandas as pd\nimport random\nimport pdfplumber\nimport re\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom langchain_community.embeddings import FastEmbedEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n# from langchain.chains.base import Chain\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.docstore.document import Document\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.prompts import PromptTemplate\nfrom langchain_groq import ChatGroq\nfrom langchain.chains import RetrievalQA\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom IPython.display import Markdown, display\n# from langchain.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain.storage import InMemoryStore\nfrom tqdm.autonotebook import tqdm, trange","metadata":{"id":"PttkDKfSNd7G","execution":{"iopub.status.busy":"2024-06-30T07:34:49.964318Z","iopub.execute_input":"2024-06-30T07:34:49.965143Z","iopub.status.idle":"2024-06-30T07:34:51.855716Z","shell.execute_reply.started":"2024-06-30T07:34:49.965105Z","shell.execute_reply":"2024-06-30T07:34:51.854785Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Environment Variables","metadata":{"id":"uKrhDUh5Nd7H"}},{"cell_type":"code","source":"# AEBS PDFs\npath1 = \"/kaggle/input/pdffiles/GB AEBS.pdf\"\npath2 = \"/kaggle/input/pdffiles/UN AEBS.pdf\"\n\n# Light PDFs\npath3 = \"/kaggle/input/pdffiles/GB Lighting installation.pdf\"\npath4 = \"/kaggle/input/pdffiles/R048r12e.pdf\"","metadata":{"id":"PmoHv-vSNd7J","outputId":"d62ac22f-fe33-459f-cf22-b75707b4aeef","execution":{"iopub.status.busy":"2024-06-30T07:34:51.856919Z","iopub.execute_input":"2024-06-30T07:34:51.857301Z","iopub.status.idle":"2024-06-30T07:34:51.861680Z","shell.execute_reply.started":"2024-06-30T07:34:51.857274Z","shell.execute_reply":"2024-06-30T07:34:51.860809Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# LLMs\nmixtral = ChatGroq(groq_api_key =\"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\", model = 'mixtral-8x7b-32768', temperature=0.05)\nllama3 = ChatGroq(groq_api_key =\"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\", model = 'llama3-70b-8192', temperature=0.05)\n\n# For question answering\ntemplate1 = \"\"\"\nYou are the Vehicle Regulation Assistant, a helpful AI assistant. Your task is to answer given questions from the provided relevant part of the PDF. The answer should be highly-detailed and well-sturctured. If possible, refer to specific sections number within the context (e.g., \"According to section 4.1.2,...\"). Do not begin your response with phrases like \"Based on the provided context, the answer to the question is:\". If the context does not contain information related to the question, explicitly state that there is no relevant information in the provided context. Be polite and helpful.\n\nCONTEXT: {context}\n\nQUESTION: {question}\n\"\"\"\ntemplate2 = \"\"\"\nYour task is to answer the question accurately and in detail, using only the information provided in the given context. Where applicable, refer to specific section numbers within the context (e.g., \"According to section 4.1.2,...\"). If the answer is not found in the provided context, simply state that there is no relevant information available without sharing details about the context.\n\nCONTEXT: {context}\n\nQUESTION: {question}\n\"\"\"\n# Avoid unnecessary phrases like \"Based on the provided context, the answer to the question is:\".\n\ntemplate3 = \"\"\"\nPlease provide a detailed and well-structured response to the question below, using only the information provided in the context.If the context does not contain information related to the question, explicitly state that there is no relevant information in the provided context. Be polite and helpful.Also, provide a confidence level from 0 to 100% in your response based on how certain you are about the information you have provided.\n\nCONTEXT: {context}\n\nQUESTION: {question}\n\"\"\"\nprompt = PromptTemplate(template=template2, input_variables=[\"question\", \"context\"])\n\ncombine_template = \"\"\"\nYour task is to answer the question accurately and in detail, by synthesizing relevant information from the provided answers.\nWhere applicable, refer to specific section numbers within the context (e.g., \"According to section 4.1.2,...\").\nDo not reveal that the information comes from multiple answers, directly answer the question.\n\nQUESTION: {question}\n\nANSWER 1: {answer1}\n\nANSWER 2: {answer2}\n\"\"\"\n# For comparison RAG 1\ncomparison_template = \"\"\"\nWe have provided a question and their two answers. Generate a comparison section without a heading which includes whether both answer are same or partially same or different. If they are paritially same, then what is same and what is different. This comparison is based on the answers generated from both the contexts. Accuracy and precision are crucial for this task.\n\nQUESTION: {question}\n\nANSWER 1: {answer1}\n\nANSWER 2: {answer2}\n\"\"\"\n# For comparison RAG 2\ndef get_comparision_prompt(query, context1, context2):\n    comparison_template = \"\"\"\n    Response in three sections\n    \n    ANSWER 1: This is firts section, here answer the question form the context 1.\n    \n    ANSWER 2: This is second section, here answer the question form the context 2.\n    \n    COMPARISON: This is third section, here answer whether both answer are same or partially same or different. If they are paritially same, then what is same and what is different.\n    This section is completely based on answer generated in first and second section.\n    \n    Please answer the question solely based on the provided context. If you can't answer any of the both questions from their context then just tell that there is no answer in that context. This is very important for my life, be very precised and accurate in answering the question and also in comparison..\n\n    QUESTION: {question}\n\n    CONTEXT1: {context1}\n\n    CONTEXT2: {context2}\n    \"\"\"\n    comparison_prompt = comparison_template.format(context1 = context1,context2 = context2, question = query)\n    return comparison_prompt\n\n# Lightning\nqueries = [\"Whats the difference between Grouped and Combined lamps?\", \"Can dipped-beam headlamp and main-beam headlamp for front lighting system?\", \"what is color of End Outline marker lamp?\", \"Can yellow lamp used as front fog lamp?\", \"Can red color light placed in the front of the vehicle?\", \"Can white light can be placed at the back of the vehicle?\", \"What are 1,1,a,1b,2a,2b,5,6 in direction indicator lamps?\", \"is cornering lamp mandatory?\", \"does reflective tape come under light and light signalling?\", \"standard weight of a person for testing?\",\"can dipped beam uses as a main beam?\", \"what are the light functions to be kept rear of the vehicle?\", \"What lamp should be fitted for passenger vehicles?\"]","metadata":{"id":"PmoHv-vSNd7J","outputId":"d62ac22f-fe33-459f-cf22-b75707b4aeef","execution":{"iopub.status.busy":"2024-06-30T07:51:54.360753Z","iopub.execute_input":"2024-06-30T07:51:54.361336Z","iopub.status.idle":"2024-06-30T07:51:54.413688Z","shell.execute_reply.started":"2024-06-30T07:51:54.361305Z","shell.execute_reply":"2024-06-30T07:51:54.412955Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Embedding Model\n# embedding_model = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-large-en\",\n#                                model_kwargs={'device': 'cuda'},\n#                                encode_kwargs={'normalize_embeddings': False})","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:34:52.337377Z","iopub.execute_input":"2024-06-30T07:34:52.337665Z","iopub.status.idle":"2024-06-30T07:34:52.341539Z","shell.execute_reply.started":"2024-06-30T07:34:52.337640Z","shell.execute_reply":"2024-06-30T07:34:52.340667Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Section wise chunking\n- after removing header footer","metadata":{}},{"cell_type":"code","source":"embedding_model= FastEmbedEmbeddings()","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:34:52.342975Z","iopub.execute_input":"2024-06-30T07:34:52.343307Z","iopub.status.idle":"2024-06-30T07:34:53.885435Z","shell.execute_reply.started":"2024-06-30T07:34:52.343276Z","shell.execute_reply":"2024-06-30T07:34:53.884659Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cf6b38be0a444e29ad16b46ce06827e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"546ba17ecf1b4df7bd0beaf3bbb8f145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/706 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d65a31514fec4e71ad5bd693da8306b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d706c7d8460f4ac68bf4f7dfc83e5dbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_optimized.onnx:   0%|          | 0.00/66.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b2932968a534778b951cd613d1387f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5c26fc3bff54969b0ba3e127d945a96"}},"metadata":{}}]},{"cell_type":"code","source":"def embed_texts(texts):\n    return FastEmbedEmbeddings.embed_documents(embedding_model,texts = texts)\n\ndef get_header_footer(pdf_path, threshold=0.71):\n    with pdfplumber.open(pdf_path) as pdf:\n        random_page_nos = random.sample(range(5, len(pdf.pages)), 10)\n        \n        avg_similarity = 1\n        header_lines = -1\n        \n        while avg_similarity > threshold and header_lines < 4:\n            header_lines += 1\n            five_lines = []\n            \n            for page_no in random_page_nos:\n                lines = pdf.pages[page_no].extract_text().split('\\n')\n                if len(lines) > header_lines:\n                    five_lines.append(lines[header_lines])\n            similarities = cosine_similarity(embed_texts(five_lines))\n            avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n            \n        avg_similarity = 1\n        footer_lines = -1\n        \n        while avg_similarity > threshold and footer_lines < 4:\n            footer_lines += 1\n            five_lines = []\n            \n            for page_no in random_page_nos:\n                lines = pdf.pages[page_no].extract_text().split('\\n')\n                if len(lines) > footer_lines:\n                    five_lines.append(lines[-(footer_lines+1)])\n            similarities = cosine_similarity(embed_texts(five_lines))\n            avg_similarity = np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n            \n        return header_lines, footer_lines\n    \ndef extract_text(pdf_path):\n    header_lines, footer_lines = get_header_footer(pdf_path)\n    with pdfplumber.open(pdf_path) as pdf:\n        text = ''\n        for page in pdf.pages:\n            page_text = page.extract_text()\n            if page_text:\n                lines = page_text.split('\\n')\n                if lines:\n                    page_text = '\\n'.join(lines[header_lines:-(footer_lines+1)])\n                    text += page_text + '\\n'\n        return text","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:34:53.886545Z","iopub.execute_input":"2024-06-30T07:34:53.886832Z","iopub.status.idle":"2024-06-30T07:34:53.898977Z","shell.execute_reply.started":"2024-06-30T07:34:53.886807Z","shell.execute_reply":"2024-06-30T07:34:53.898225Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(\"approximate no. of tokens\", len(extract_text(path4).split()))","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:34:53.900211Z","iopub.execute_input":"2024-06-30T07:34:53.900499Z","iopub.status.idle":"2024-06-30T07:35:20.621545Z","shell.execute_reply.started":"2024-06-30T07:34:53.900475Z","shell.execute_reply":"2024-06-30T07:35:20.620637Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"approximate no. of tokens 39382\n","output_type":"stream"}]},{"cell_type":"code","source":"pattern = re.compile(r'\\n([1-9]|1[0-9])\\. [A-Z][a-zA-Z]+')   #United nations\n# pattern = re.compile('\\d\\. [A-Z]')                          # Smb United nations\n# pattern = re.compile(r'\\n([1-9]|1[0-9]) [A-Z][a-zA-Z]+')    #Chinese\n# pattern = re.compile(r'(\\n(1[0-9]|[1-9])\\s+[A-Z][a-zA-Z]+.*?)(?=\\n(?:[1-9]|1[0-5])\\s+[A-Z]|$)', re.DOTALL) #Chinese","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:35:20.622896Z","iopub.execute_input":"2024-06-30T07:35:20.623355Z","iopub.status.idle":"2024-06-30T07:35:20.628488Z","shell.execute_reply.started":"2024-06-30T07:35:20.623321Z","shell.execute_reply":"2024-06-30T07:35:20.627407Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def section_wise_chunking(pdf_path):\n    text = extract_text(pdf_path)\n    matches = list(pattern.finditer(text))\n    \n    # Use the positions of the matches to split the text into sections\n    sections = []\n    last_index = 0\n    for match in matches:\n        start, end = match.span()\n        section_text = text[last_index:start].strip()\n        if section_text:\n            sections.append(section_text)\n        last_index = start\n    if last_index < len(text):\n        sections.append(text[last_index:].strip())\n    \n    # Handeling too small and too large sections\n    text_chunks = []\n    for i, section in enumerate(sections):\n        if i != 0 and (len(section.split()) < 400):\n            text_chunks[-1] += \"\\n\"+section\n        elif len(section.split()) > 800:\n            splitted_chunks = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=300).split_text(section)\n            text_chunks += splitted_chunks[:1] + [splitted_chunks[0].split('\\n')[0]+' (Partial)\\n'+ chunk for chunk in splitted_chunks[1:]]\n        else:\n            text_chunks.append(section)\n    return text_chunks","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:35:20.632318Z","iopub.execute_input":"2024-06-30T07:35:20.632618Z","iopub.status.idle":"2024-06-30T07:35:20.642379Z","shell.execute_reply.started":"2024-06-30T07:35:20.632595Z","shell.execute_reply":"2024-06-30T07:35:20.641481Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(\"chunking....\")\nchunks = section_wise_chunking(path4)\nprint(\"No. of chunks created:\", len(chunks))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:35:20.644039Z","iopub.execute_input":"2024-06-30T07:35:20.644433Z","iopub.status.idle":"2024-06-30T07:35:47.407889Z","shell.execute_reply.started":"2024-06-30T07:35:20.644401Z","shell.execute_reply":"2024-06-30T07:35:47.406976Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"chunking....\nNo. of chunks created: 53\n","output_type":"stream"}]},{"cell_type":"code","source":"# print(\"No. of chunks:\", len(chunks))\n# for i, chunk in enumerate(chunks):\n#     print(chunk)\n#     print(\"\\n\" + \"_\"*80,len(chunk.split()), \"Words\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:35:47.409005Z","iopub.execute_input":"2024-06-30T07:35:47.409286Z","iopub.status.idle":"2024-06-30T07:35:47.413313Z","shell.execute_reply.started":"2024-06-30T07:35:47.409262Z","shell.execute_reply":"2024-06-30T07:35:47.412420Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Get vectorstore\n- FAISS.from_documents takes list of docs as argument\n","metadata":{"id":"16NqA0VpNd7K"}},{"cell_type":"code","source":"# for RecursiveCharacterTextSplitter\ndef get_vectorstore1(path):\n    text = extract_text(path)\n    texts = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_text(text)\n    docs = [Document(text) for text in texts if text.strip()]\n#     docs = PyPDFLoader(path).load_and_split(RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True))\n    vectorstore = FAISS.from_documents(docs, embedding_model)\n    return vectorstore","metadata":{"id":"sbHLL94jNd7L","execution":{"iopub.status.busy":"2024-06-30T07:40:42.397892Z","iopub.execute_input":"2024-06-30T07:40:42.398242Z","iopub.status.idle":"2024-06-30T07:40:42.403543Z","shell.execute_reply.started":"2024-06-30T07:40:42.398217Z","shell.execute_reply":"2024-06-30T07:40:42.402668Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# for section_wise_chunking\ndef get_vectorstore2(path):\n    texts = section_wise_chunking(path)\n    docs = [Document(text) for text in texts if text.strip()]\n    vectorstore = FAISS.from_documents(docs, embedding_model)\n    return vectorstore","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:40:42.877683Z","iopub.execute_input":"2024-06-30T07:40:42.877997Z","iopub.status.idle":"2024-06-30T07:40:42.882814Z","shell.execute_reply.started":"2024-06-30T07:40:42.877971Z","shell.execute_reply":"2024-06-30T07:40:42.881955Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Double retrieval RAG","metadata":{}},{"cell_type":"code","source":"%%time\nretriever1 = get_vectorstore1(path4).as_retriever(search_kwargs={\"k\": 5})","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:40:45.837978Z","iopub.execute_input":"2024-06-30T07:40:45.838973Z","iopub.status.idle":"2024-06-30T07:42:29.491918Z","shell.execute_reply.started":"2024-06-30T07:40:45.838930Z","shell.execute_reply":"2024-06-30T07:42:29.490977Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"CPU times: user 3min, sys: 599 ms, total: 3min\nWall time: 1min 43s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nretriever2 = get_vectorstore2(path4).as_retriever()","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:39:25.773124Z","iopub.execute_input":"2024-06-30T07:39:25.773489Z","iopub.status.idle":"2024-06-30T07:40:03.735912Z","shell.execute_reply.started":"2024-06-30T07:39:25.773442Z","shell.execute_reply":"2024-06-30T07:40:03.734979Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"CPU times: user 49.6 s, sys: 73.6 ms, total: 49.7 s\nWall time: 38 s\n","output_type":"stream"}]},{"cell_type":"code","source":"def Double_RAG(query):\n    \n    chain1 = RetrievalQA.from_llm(llm=llama3, retriever=retriever1, prompt= prompt)\n    chain2 = RetrievalQA.from_llm(llm=llama3, retriever=retriever2, prompt= prompt)\n    \n    answer1 = chain1.invoke(query)['result']\n    answer2 = chain2.invoke(query)['result']\n    \n    print(\"ANSWER1:\", answer1)\n    print(\"ANSWER2:\", answer2)\n\n    combine_prompt = combine_template.format(question = query,answer1 = answer1, answer2 = answer2)\n    response = llama3.invoke(combine_prompt).content\n    \n    return response","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:53:01.144927Z","iopub.execute_input":"2024-06-30T07:53:01.145298Z","iopub.status.idle":"2024-06-30T07:53:01.151889Z","shell.execute_reply.started":"2024-06-30T07:53:01.145267Z","shell.execute_reply":"2024-06-30T07:53:01.150866Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"saved_responses = []\nfor query in queries:\n    print(\"QUERY:\",query)\n    print(\"---- RecursiveCharacterTextSplitter tokens:\",[len(retriever1.invoke(query)[i].page_content.split()) for i in range(5)])\n    print(\"---- section_wise_chunking tokens:\",[len(retriever2.invoke(query)[i].page_content.split()) for i in range(4)])\n    s = time.time()\n    response = Double_RAG(query) \n    print(\"COMBINED ANSWER:\")\n    display(Markdown(response))\n    print(\"_\"*60, \" generated in\", time.time()-s)\n    saved_responses.append({\"query\": query, \"response\": response})","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:58:27.552970Z","iopub.execute_input":"2024-06-30T07:58:27.553725Z","iopub.status.idle":"2024-06-30T08:12:35.754476Z","shell.execute_reply.started":"2024-06-30T07:58:27.553693Z","shell.execute_reply":"2024-06-30T08:12:35.753652Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"QUERY: Whats the difference between Grouped and Combined lamps?\n---- RecursiveCharacterTextSplitter tokens: [140, 150, 151, 149, 161]\n---- section_wise_chunking tokens: [752, 784, 756, 789]\nANSWER1: According to section 2.7.4 and 2.7.5 of the context, the difference between Grouped and Combined lamps is as follows:\n\n* Grouped lamps (2.7.4) are devices having separate apparent surfaces in the direction of the reference axis and separate light sources, but a common lamp body.\n* Combined lamps (2.7.5) are devices having separate apparent surfaces in the direction of the reference axis, but a common light source and a common lamp body.\n\nIn other words, Grouped lamps have separate light sources, while Combined lamps share a common light source.\nANSWER2: According to the provided context, specifically section 2.7.3 and 2.7.5, the difference between Grouped and Combined lamps is as follows:\n\n**Grouped lamps** (section 2.7.3) are devices having separate apparent surfaces in the direction of the reference axis and separate light sources, but a common lamp body.\n\n**Combined lamps** (section 2.7.5) are devices having separate apparent surfaces in the direction of the reference axis, but a common light source and a common lamp body.\n\nIn summary, the key difference is that Grouped lamps have separate light sources, while Combined lamps share a common light source.\nCOMBINED ANSWER:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"The difference between Grouped and Combined lamps lies in the number of light sources they have. Grouped lamps are devices that have separate apparent surfaces in the direction of the reference axis, separate light sources, but a common lamp body. On the other hand, Combined lamps are devices that have separate apparent surfaces in the direction of the reference axis, but a common light source and a common lamp body. In other words, Grouped lamps have multiple light sources, one for each apparent surface, whereas Combined lamps have a single light source that serves all apparent surfaces."},"metadata":{}},{"name":"stdout","text":"____________________________________________________________  generated in 10.609696865081787\nQUERY: Can dipped-beam headlamp and main-beam headlamp for front lighting system?\n---- RecursiveCharacterTextSplitter tokens: [151, 138, 145, 141, 142]\n---- section_wise_chunking tokens: [820, 799, 784, 762]\nANSWER1: According to section 6.2.7.2, \"The dipped-beam may remain switched on at the same time as the main beams.\" This implies that it is allowed to have both dipped-beam headlamps and main-beam headlamps switched on simultaneously for the front lighting system.\nANSWER2: According to the provided context, the answer is yes. \n\nIn section 6.1.7.4, it is stated that \"The main-beam headlamps may be switched on either simultaneously or in pairs.\" and \"For changing over from the dipped to the main beam at least one pair of main-beam headlamps shall be switched on.\" This implies that both dipped-beam headlamps and main-beam headlamps can be used for front lighting systems.\n\nAdditionally, in section 6.2.7.2, it is mentioned that \"The dipped-beam may remain switched on at the same time as the main beams.\" This further supports the idea that both dipped-beam headlamps and main-beam headlamps can be used simultaneously for front lighting systems.\nCOMBINED ANSWER:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Yes, it is allowed to have both dipped-beam headlamps and main-beam headlamps for the front lighting system. According to section 6.2.7.2, the dipped-beam may remain switched on at the same time as the main beams, implying that it is permissible to have both types of headlamps switched on simultaneously. Furthermore, section 6.1.7.4 states that the main-beam headlamps may be switched on either simultaneously or in pairs, and that at least one pair of main-beam headlamps shall be switched on when changing over from the dipped to the main beam. This suggests that both dipped-beam headlamps and main-beam headlamps can be used for front lighting systems, and that they can be used together or separately as needed."},"metadata":{}},{"name":"stdout","text":"____________________________________________________________  generated in 69.77866220474243\nQUERY: what is color of End Outline marker lamp?\n---- RecursiveCharacterTextSplitter tokens: [156, 158, 142, 170, 164]\n---- section_wise_chunking tokens: [756, 792, 810, 802]\nANSWER1: According to section 5.15, the color of the End-outline marker lamp is White in front and Red at the rear.\nANSWER2: According to the provided context, the color of the End Outline marker lamp is not explicitly specified. However, it is mentioned in section 5 that \"Conspicuity marking: White to the front; White or yellow to the side; Red or yellow to the rear.\" Since the End Outline marker lamp is a type of conspicuity marking, it can be inferred that it may be white or yellow, but the exact color is not specified.\nCOMBINED ANSWER:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"According to section 5.15, the color of the End-outline marker lamp is White in front and Red at the rear."},"metadata":{}},{"name":"stdout","text":"____________________________________________________________  generated in 67.16660213470459\nQUERY: Can yellow lamp used as front fog lamp?\n---- RecursiveCharacterTextSplitter tokens: [162, 161, 157, 167, 142]\n---- section_wise_chunking tokens: [784, 756, 789, 1095]\nANSWER1: According to the provided context, there is no relevant information available that suggests a yellow lamp can be used as a front fog lamp. The context only mentions the orientation, vertical inclination, and electrical connections of front fog lamps, but it does not specify the color of the lamp. Therefore, it cannot be determined from the provided context whether a yellow lamp can be used as a front fog lamp.\nANSWER2: According to section 5.15 of the provided context, a front fog lamp can emit either white or selective yellow light. Therefore, a yellow lamp can be used as a front fog lamp.\nCOMBINED ANSWER:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"According to section 5.15, a front fog lamp can emit either white or selective yellow light, which implies that a yellow lamp can be used as a front fog lamp."},"metadata":{}},{"name":"stdout","text":"____________________________________________________________  generated in 71.42583799362183\nQUERY: Can red color light placed in the front of the vehicle?\n---- RecursiveCharacterTextSplitter tokens: [168, 142, 151, 170, 131]\n---- section_wise_chunking tokens: [802, 756, 833, 788]\nANSWER1: According to section 5.10.1, \"For the visibility of red light towards the front of a vehicle, with the exception of a red rearmost side-marker lamp, there shall be no direct visibility of the apparent surface of a red lamp if viewed by an observer moving within Zone 1 as specified in Annex 4.\"\n\nThis implies that red color light cannot be placed in the front of the vehicle, except for a red rearmost side-marker lamp.\nANSWER2: According to the provided context, there is no specific information that prohibits the use of red color light in the front of a vehicle. However, it is mentioned in section 5 of the context that \"Conspicuity marking: White to the front; White or yellow to the side; Red or yellow to the rear.\" This suggests that red color is not typically used for conspicuity markings in the front of a vehicle, but it does not explicitly prohibit its use.\n\nAdditionally, section 6.9.9.1 of the context mentions that infrared radiation generators are allowed to be activated only when the headlamp on the same side of the vehicle is switched on and the vehicle is in forward motion. This implies that some form of lighting or illumination is allowed in the front of the vehicle, but again, it does not specifically address the use of red color light.\n\nIn summary, while there is no explicit prohibition on using red color light in the front of a vehicle, the context does not provide clear guidance or permission for its use either.\nCOMBINED ANSWER:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"According to section 5.10.1, red color light cannot be placed in the front of a vehicle, with the exception of a red rearmost side-marker lamp. This implies that direct visibility of a red lamp towards the front of a vehicle is not allowed, as viewed by an observer moving within Zone 1 as specified in Annex 4. While section 5 suggests that red color is not typically used for conspicuity markings in the front of a vehicle, it does not explicitly prohibit its use. However, the explicit restriction in section 5.10.1 takes precedence, indicating that red color light should not be placed in the front of a vehicle, except for the specified exception."},"metadata":{}},{"name":"stdout","text":"____________________________________________________________  generated in 70.59026646614075\nQUERY: Can white light can be placed at the back of the vehicle?\n---- RecursiveCharacterTextSplitter tokens: [142, 150, 168, 151, 170]\n---- section_wise_chunking tokens: [756, 802, 788, 807]\nANSWER1: According to section 5.10.2, \"For the visibility of white light towards the rear, with the exception of reversing lamps and white side conspicuity markings fitted to the vehicle, there shall be no direct visibility of the apparent surface of a white lamp if viewed by an observer moving within Zone 2 in a transverse plane situated 25 m behind the vehicle (see Annex 4)\". This implies that white light is not allowed to be placed at the back of the vehicle, except for reversing lamps and white side conspicuity markings.\nANSWER2: According to section 5 of the provided context, \"Conspicuity marking: ... Red or yellow to the rear.\" This implies that white light is not allowed at the back of the vehicle.\nCOMBINED ANSWER:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"According to the regulations, white light cannot be placed at the back of the vehicle, with the exception of reversing lamps and white side conspicuity markings. This is because, as stated in section 5.10.2, there shall be no direct visibility of the apparent surface of a white lamp if viewed by an observer moving within Zone 2 in a transverse plane situated 25 m behind the vehicle. Additionally, section 5 of the context specifies that conspicuity markings on the rear of the vehicle should be red or yellow, further supporting the notion that white light is not permitted at the back of the vehicle."},"metadata":{}},{"name":"stdout","text":"____________________________________________________________  generated in 66.83231687545776\nQUERY: What are 1,1,a,1b,2a,2b,5,6 in direction indicator lamps?\n---- RecursiveCharacterTextSplitter tokens: [158, 179, 192, 151, 170]\n---- section_wise_chunking tokens: [876, 827, 789, 756]\nANSWER1: According to the provided context, 1, 1a, 1b, 2a, 2b, 5, and 6 refer to categories of direction-indicator lamps.\n\nSpecifically:\n\n* Categories 1, 1a, and 1b refer to front direction-indicator lamps (section 6.5.3).\n* Categories 2a and 2b refer to rear direction-indicator lamps (section 6.5.3).\n* Categories 5 and 6 refer to side direction-indicator lamps (section 6.5.3).\n\nThese categories are used to define the arrangement and requirements for direction-indicator lamps on vehicles.\nANSWER2: According to the provided context, 1, 1a, 1b, 2a, 2b, 5, and 6 are categories of direction-indicator lamps.\n\nSpecifically, they refer to the following:\n\n* 1, 1a, and 1b: Front direction-indicator lamp categories\n* 2a and 2b: Rear direction-indicator lamp categories\n* 5 and 6: Side direction-indicator lamp categories\nCOMBINED ANSWER:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"In direction indicator lamps, the numbers 1, 1a, 1b, 2a, 2b, 5, and 6 refer to specific categories of direction-indicator lamps. These categories define the arrangement and requirements for direction-indicator lamps on vehicles. Specifically, categories 1, 1a, and 1b refer to front direction-indicator lamps, categories 2a and 2b refer to rear direction-indicator lamps, and categories 5 and 6 refer to side direction-indicator lamps. These categories are used to ensure that direction-indicator lamps are properly installed and function correctly on vehicles."},"metadata":{}},{"name":"stdout","text":"____________________________________________________________  generated in 70.31753826141357\nQUERY: is cornering lamp mandatory?\n---- RecursiveCharacterTextSplitter tokens: [154, 143, 153, 149, 137]\n---- section_wise_chunking tokens: [789, 802, 833, 756]\nANSWER1: According to section 6.20.1, the presence of a cornering lamp is optional on motor vehicles. Therefore, the answer is no, the cornering lamp is not mandatory.\nANSWER2: According to section 6.20.1, the presence of a cornering lamp is optional on motor vehicles.\nCOMBINED ANSWER:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"According to section 6.20.1, the presence of a cornering lamp is optional on motor vehicles. Therefore, the answer is no, the cornering lamp is not mandatory."},"metadata":{}},{"name":"stdout","text":"____________________________________________________________  generated in 67.57276678085327\nQUERY: does reflective tape come under light and light signalling?\n---- RecursiveCharacterTextSplitter tokens: [167, 174, 145, 168, 160]\n---- section_wise_chunking tokens: [783, 784, 784, 756]\nANSWER1: According to the provided context, there is no relevant information available that directly answers the question of whether reflective tape comes under light and light signalling. The context primarily focuses on lamps, light-signalling devices, and their photometric requirements, but it does not mention reflective tape.\nANSWER2: Based on the provided context, reflective tape or conspicuity markings are mentioned in Section 5.15, which lists the colors of light emitted by various lamps. Conspicuity markings are specified as having the following colors: White to the front, White or yellow to the side, and Red or yellow to the rear.\n\nHowever, conspicuity markings are not considered as light sources or light-signalling devices. They are passive reflective materials that reflect light from other sources, such as headlights, to increase visibility.\n\nIn the context of light-signalling devices, Section 5.15 lists various lamps and their corresponding colors, but conspicuity markings are not included in this list.\n\nTherefore, reflective tape or conspicuity markings do not come under light and light signalling in the context of this regulation.\nCOMBINED ANSWER:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Reflective tape, also referred to as conspicuity markings, does not come under light and light signalling. While it is mentioned in Section 5.15, it is not considered a light source or light-signalling device. Instead, it is a passive reflective material that reflects light from other sources, such as headlights, to increase visibility. The colors of conspicuity markings are specified as white to the front, white or yellow to the side, and red or yellow to the rear. In contrast, light and light signalling refer to lamps and light-signalling devices that emit light, which are listed in Section 5.15, but do not include conspicuity markings."},"metadata":{}},{"name":"stdout","text":"____________________________________________________________  generated in 66.5406403541565\nQUERY: standard weight of a person for testing?\n---- RecursiveCharacterTextSplitter tokens: [206, 161, 153, 156, 194]\n---- section_wise_chunking tokens: [535, 791, 1095, 810]\nANSWER1: According to paragraph 1 of the context, the standard weight of a person for testing is 75 kg.\nANSWER2: There is no relevant information available in the provided context about the standard weight of a person for testing.\nCOMBINED ANSWER:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"According to the available information, the standard weight of a person for testing is 75 kg."},"metadata":{}},{"name":"stdout","text":"____________________________________________________________  generated in 68.38029956817627\nQUERY: can dipped beam uses as a main beam?\n---- RecursiveCharacterTextSplitter tokens: [151, 151, 142, 145, 141]\n---- section_wise_chunking tokens: [799, 820, 783, 784]\nANSWER1: According to the provided context, there is no information that suggests a dipped beam can be used as a main beam. In fact, the context consistently distinguishes between dipped-beam headlamps and main-beam headlamps, and provides separate regulations for each.\n\nFor example, section 6.2.7.1 states that \"The control for changing over to the dipped-beam shall switch off all main-beam headlamps simultaneously.\" This implies that the dipped beam and main beam are two separate entities that cannot be used interchangeably.\n\nAdditionally, section 6.1.7.4 states that \"For changing over from the dipped to the main beam at least one pair of main-beam headlamps shall be switched on. For changing over from the main-beam to the dipped-beam all main-beam headlamps shall be switched off simultaneously.\" This further reinforces the idea that dipped beam and main beam are distinct and cannot be used as substitutes for each other.\n\nTherefore, based on the provided context, it appears that a dipped beam cannot be used as a main beam.\nANSWER2: According to section 6.2.7.2, \"The dipped-beam may remain switched on at the same time as the main-beams.\" This implies that a dipped-beam headlamp can be used simultaneously with a main-beam headlamp, but it does not explicitly state that a dipped-beam can be used as a main beam. \n\nHowever, it is important to note that the main purpose of a dipped-beam headlamp is to provide a lower beam of light that does not dazzle oncoming traffic, whereas a main-beam headlamp is designed to provide a higher beam of light for better visibility at higher speeds. Using a dipped-beam as a main beam might not provide the same level of visibility and could potentially cause glare for oncoming traffic.\n\nIn summary, while the regulation does not explicitly prohibit using a dipped-beam as a main beam, it is not recommended as it may not provide the same level of visibility and could cause glare for oncoming traffic.\nCOMBINED ANSWER:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Based on the provided context, it appears that a dipped beam cannot be used as a main beam. The regulations consistently distinguish between dipped-beam headlamps and main-beam headlamps, and provide separate regulations for each. For example, section 6.2.7.1 states that the control for changing over to the dipped-beam shall switch off all main-beam headlamps simultaneously, implying that the dipped beam and main beam are two separate entities that cannot be used interchangeably.\n\nAdditionally, the main purpose of a dipped-beam headlamp is to provide a lower beam of light that does not dazzle oncoming traffic, whereas a main-beam headlamp is designed to provide a higher beam of light for better visibility at higher speeds. Using a dipped-beam as a main beam might not provide the same level of visibility and could potentially cause glare for oncoming traffic.\n\nAlthough section 6.2.7.2 allows the dipped-beam to remain switched on at the same time as the main-beams, it does not explicitly state that a dipped-beam can be used as a main beam. Therefore, based on the provided context, it is not recommended to use a dipped-beam as a main beam, as it may not provide the same level of visibility and could cause glare for oncoming traffic."},"metadata":{}},{"name":"stdout","text":"____________________________________________________________  generated in 75.6738498210907\nQUERY: what are the light functions to be kept rear of the vehicle?\n---- RecursiveCharacterTextSplitter tokens: [151, 153, 156, 150, 170]\n---- section_wise_chunking tokens: [802, 756, 819, 770]\nANSWER1: According to the provided context, the light functions to be kept rear of the vehicle are:\n\n* Rear position lamp (Section 6.10)\n* Rear light-signalling devices (Section 6.1.7.3)\n* End-outline marker lamp (Section 6.13)\n\nNote that these sections provide specific requirements and regulations for these light functions, including their orientation, electrical connections, and tell-tale indicators.\nANSWER2: According to the provided context, the light functions to be kept at the rear of the vehicle are:\n\n1. Rear position lamps (Regulation No. 7) - mandatory on motor vehicles and trailers (Section 6.10).\n2. Rear fog lamp (Regulation No. 38) - mandatory on motor vehicles, optional on trailers (Section 6.11).\n3. Rear retro-reflector, non-triangular (Regulation No. 3) - mandatory on motor vehicles, optional on trailers (Section 6.14).\n4. End-outline marker lamp (Regulation No. 7) - mandatory on vehicles exceeding 2.10 m in width, optional on vehicles between 1.80 and 2.10 m in width, and on chassis-cabs (Section 6.13).\n5. Parking lamp (Regulation No. 77 or 7) - optional on motor vehicles not exceeding 6 m in length and not exceeding 2 m in width, prohibited on all other vehicles (Section 6.12).\nCOMBINED ANSWER:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"The light functions to be kept rear of the vehicle are:\n\n* Rear position lamp (mandatory on motor vehicles and trailers)\n* Rear fog lamp (mandatory on motor vehicles, optional on trailers)\n* Rear retro-reflector, non-triangular (mandatory on motor vehicles, optional on trailers)\n* End-outline marker lamp (mandatory on vehicles exceeding 2.10 m in width, optional on vehicles between 1.80 and 2.10 m in width, and on chassis-cabs)\n* Rear light-signalling devices\n* Parking lamp (optional on motor vehicles not exceeding 6 m in length and not exceeding 2 m in width, prohibited on all other vehicles)\n\nThese light functions are subject to specific requirements and regulations, including their orientation, electrical connections, and tell-tale indicators, as outlined in the relevant sections."},"metadata":{}},{"name":"stdout","text":"____________________________________________________________  generated in 69.54209089279175\nQUERY: What lamp should be fitted for passenger vehicles?\n---- RecursiveCharacterTextSplitter tokens: [170, 142, 158, 148, 164]\n---- section_wise_chunking tokens: [770, 802, 788, 756]\nANSWER1: According to the provided context, there is no specific requirement for a particular lamp to be fitted for passenger vehicles. However, it can be inferred that certain lamps are optional or mandatory depending on the vehicle's dimensions and category.\n\nFor example, parking lamps are optional for motor vehicles not exceeding 6 m in length and not exceeding 2 m in width (Section 6.12.1). End-outline marker lamps are mandatory for vehicles exceeding 2.10 m in width and optional for vehicles between 1.80 and 2.10 m in width (Section 6.13.1).\n\nIt is also important to note that the context provides requirements for various lamps, such as rear fog-lamps, stop-lamps, and courtesy lamps, but it does not specify a particular lamp that must be fitted for passenger vehicles.\nANSWER2: There is no specific information in the provided context about what lamp should be fitted for passenger vehicles. The context provides general specifications and individual specifications for various lamps, but it does not provide a direct answer to this question.\nCOMBINED ANSWER:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Based on the provided context, there is no specific requirement for a particular lamp to be fitted for passenger vehicles. However, certain lamps are optional or mandatory depending on the vehicle's dimensions and category. For instance, parking lamps are optional for motor vehicles not exceeding 6 m in length and not exceeding 2 m in width (Section 6.12.1), while end-outline marker lamps are mandatory for vehicles exceeding 2.10 m in width and optional for vehicles between 1.80 and 2.10 m in width (Section 6.13.1). The context provides general specifications and individual specifications for various lamps, such as rear fog-lamps, stop-lamps, and courtesy lamps, but it does not specify a particular lamp that must be fitted for passenger vehicles."},"metadata":{}},{"name":"stdout","text":"____________________________________________________________  generated in 67.76967453956604\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.DataFrame(saved_responses)\ndf.to_excel('responses_DRAG_detailed2.xlsx', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# query = \"Whats the difference between Grouped and Combined lamps?\"\n# for i in range(4):\n#     print(retriever2.invoke(query)[i].page_content)\n#     print(\"_\"*80)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:58:08.192023Z","iopub.execute_input":"2024-06-30T07:58:08.192766Z","iopub.status.idle":"2024-06-30T07:58:08.196515Z","shell.execute_reply.started":"2024-06-30T07:58:08.192735Z","shell.execute_reply":"2024-06-30T07:58:08.195560Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## RAG 1\n- Using FAISS retriever\n- Adv. any k\n- RetrievalQA chain","metadata":{}},{"cell_type":"code","source":"%%time\n# retriever = get_vectorstore_2(path4).as_retriever(search_kwargs={\"k\": 4})\nretriever = vectorstore_path4.as_retriever()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def RAG1(query):\n    qa_chain = RetrievalQA.from_llm(llm=llama3, retriever=retriever, prompt= prompt)\n    return qa_chain.invoke(query)['result']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\nfor query in queries4:\n    print(\"Query:\",query)\n    print(\"Tokens:\",[len(retriever.invoke(query)[i].page_content.split()) for i in range(4)])   \n    response = RAG1(query) \n    display(Markdown(response))\n    print(\"_\"*100)\n    data.append({\"query\": query, \"response\": response})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(data)\ndf.to_excel('responses10_prompt3.xlsx', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q = \"what is color of End Outline marker lamp?\"\nfor context in retriever.invoke(q):\n    print(context.page_content)\n    print(\"_\"*80)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison_RAG2\n- using PDR\n- Fixed k = 4","metadata":{}},{"cell_type":"code","source":"def PDR(path):\n    documents = PyPDFLoader(path).load()\n    combined_text = \"\\n\".join(document.page_content for document in documents)\n    document = [Document(page_content=combined_text, metadata={\"source\": path})]\n\n    retriever = ParentDocumentRetriever(vectorstore=Chroma(collection_name=\"full_documents\", embedding_function=FastEmbedEmbeddings()),\n                                        docstore=InMemoryStore(),\n                                        child_splitter=RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100),\n                                        parent_splitter=RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100))\n    \n    retriever.add_documents(document, ids=None)\n    return retriever","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nretriever2 = PDR(path4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Comparison_RAG2(query):\n    qa_chain = RetrievalQA.from_llm(llm=llama3, retriever=retriever2, prompt= prompt)\n    return qa_chain.invoke(query)['result']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\nfor query in queries4:\n    print(\"#\",query)\n    response = Comparison_RAG2(query) \n    print(response)\n    print(\"---------------------------------------------------------------------\")\n    data.append({\"query\": query, \"response\": response})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save responses and export\ndf = pd.DataFrame(data)\ndf.to_excel('query_responses10.xlsx', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison_RAG 1\n- RAG for comparison\n- by FAISS Retrieval\n- by calling LLM 3 times\n- chain - RetrievalQA","metadata":{}},{"cell_type":"code","source":"%%time\nretriever1 = get_vectorstore(path1).as_retriever(search_kwargs={\"k\": 6})\nretriever2 = get_vectorstore(path2).as_retriever(search_kwargs={\"k\": 6})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Comparison_RAG1(query):\n    llm = llama3\n    \n    qa_chain1 = RetrievalQA.from_llm(llm=llm, retriever=retriever1, prompt= prompt,)\n    qa_chain2 = RetrievalQA.from_llm(llm=llm, retriever=retriever2, prompt= prompt)\n\n    answer1 = qa_chain1.invoke(query)['result']\n    answer2 = qa_chain2.invoke(query)['result']\n    \n    comparison_prompt = comparison_template.format(question = query,answer1 = answer1, answer2 = answer2)\n    comparison = llm.invoke(comparison_prompt).content\n    \n    response = f\"**ANSWER 1**: {answer1}\\n\\n**ANSWER 2**: {answer2}\\n\\n**COMPARISION**: {comparison}\"\n    return response","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AEBS\nqueries2 = [\"Explain the test procedures in detail, in details.\", \"What are warning indications, in details.\"]\nfor query in queries2:\n    display(Markdown(Co1parison_RAG1(query)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison_RAG 2\n- by PDR\n- 3 LLM calls","metadata":{"execution":{"iopub.status.busy":"2024-06-20T17:15:16.093327Z","iopub.execute_input":"2024-06-20T17:15:16.093707Z","iopub.status.idle":"2024-06-20T17:15:16.099623Z","shell.execute_reply.started":"2024-06-20T17:15:16.093678Z","shell.execute_reply":"2024-06-20T17:15:16.098532Z"}}},{"cell_type":"code","source":"retriever1 = PDR(path1)\nretriever2 = PDR(path2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Comparison_RAG2(query):\n    context1 = get_context(query, retriever1)\n    context2 = get_context(query, retriever2)\n\n    qa_prompt1 = qa_template.format(question = query,context = context1)\n    qa_prompt2 = qa_template.format(question = query,context = context2)\n    \n    answer1 = llama3.invoke(qa_prompt1).content\n    answer2 = llama3.invoke(qa_prompt2).content\n    \n    comparison_prompt = comparison_template.format(question = query,answer1 = answer1, answer2 = answer2)\n    comparison = llm.invoke(comparison_prompt).content\n    \n    response = f\"**ANSWER 1**: {answer1}\\n\\n**ANSWER 2**: {answer2}\\n\\n**COMPARISION**: {comparison}\"\n    return response","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison_RAG 3\n- Vectostore to context from scratch`\n- Dis. - 1 prompt, 1 LLM call, too much load on geneartion","metadata":{}},{"cell_type":"code","source":"def get_context(query, vectorstore):\n    retrieved_docs = vectorstore.similarity_search_with_relevance_scores(query, k = 6)\n    context = \"\"\n    for doc in retrieved_docs:\n        context += doc.page_content\n    return context","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorstore1 = get_vectorstore(path1)\nvectorstore2 = get_vectorstore(path2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Comparison_RAG3(query):\n    context1 = get_context(query, retriever1)\n    context2 = get_context(query, retriever2)\n    \n    prompt = get_comparision_prompt(query, context1, context2)\n    \n    return llama3.invoke(prompt).content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#AEBS\nqueries1 = [\"Explain the test procedures in detail.\",\"When collision early warning signal shall be sent?\",\"What are warning indications, in details.\",\"What AEBS should do in vehicle ignition?\",\"The total speed reduction of the subject vehicle at the time of the collision with the stationary target shall be not less than how many kilometers per hour?\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for query in queries1:\n    print(\"#\",query)\n    display(Markdown(Comparison_RAG3(query)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Some notes","metadata":{}},{"cell_type":"code","source":"## FAISS\n# both dont have k as a parameters\n# Both returns list of documents retrievd\n            \n# retriever1.get_relevant_documents(query) #deprceated\n# retriever1.invoke(query) #use this instead","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retriever.invoke","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retriever.get_relevant_documents(\"degrees on a sphere\", k =1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import inspect\nsignature = inspect.signature(retriever.invoke)\n\nfor param_name, param in signature.parameters.items():\n    print(f\"Parameter: {param_name}\")\n    print(f\"  Default: {param.default}\")\n    print(f\"  Annotation: {param.annotation}\")\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import inspect\nsignature = inspect.signature(retriever.get_relevant_documents)\n\nfor param_name, param in signature.parameters.items():\n    print(f\"Parameter: {param_name}\")\n    print(f\"  Default: {param.default}\")\n    print(f\"  Annotation: {param.annotation}\")\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import inspect\nsignature = inspect.signature(retriever1.get_relevant_documents)\n\nfor param_name, param in signature.parameters.items():\n    print(f\"Parameter: {param_name}\")\n    print(f\"  Default: {param.default}\")\n    print(f\"  Annotation: {param.annotation}\")\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import inspect\nsignature = inspect.signature(get_vectorstore(path4).as_retriever)\n\nfor param_name, param in signature.parameters.items():\n    print(f\"Parameter: {param_name}\")\n    print(f\"  Default: {param.default}\")\n    print(f\"  Annotation: {param.annotation}\")\n    print()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#multivector embeddings have best performance for retrieval https://www.rungalileo.io/blog/mastering-rag-how-to-select-an-embedding-model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(llama3.invoke(\"Do you know about how Retrieval augmented generation works? You(llm) will be provided with some context from the whole context(pdf) to generate the answer, but the problem is sometimses the context extracted for you is not very relevant but we can't be sure that whether it is problem of retriever or not(It may possible that relevant information is present in the pdf , but retrievr was not able to extract them). So if the context is irravalent , you can't say the pdf doesn't provide infomation about the query. did you understand what I am saying. If I tell you in the prompt that the llm(you) are used in a rag implementation, will you able to act accordingly? Will it improve the responses?. Write a good prompt for the same issue (for the same issue i mentioned before), insuer that prompt is short and understandable by the llm\").content)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(llama3.invoke(\"Telling a llm that he(llm) is being used in a Retrieval augmented generation implementation. How thsi will affect the performanec of behaviour of llm\").content)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optional way for RAG2# def get_context(query, retriever):\n#     retrieved_docs = retriever.get_relevant_documents(query)\n#     context = \" \".join([doc.page_content for doc in retrieved_docs])\n#     return context\n\n# def RAG2(query):\n#     context = get_context(query,retriever1)\n#     qa_prompt = template2.format(question = query,context = context)\n#     return llama3.invoke(qa_prompt).content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- llama3 could be taken from groq or ollama\n- Possible raesons for latency in pdr-llama2-chromadb code\n  - llama2 downloaded in local computer\n  - slow parent document retriever\n  - small cpu\n- Giskard for evaluation\n  - By default uses gpt4 but llama3 also could be connected\n  - it finds contexts from pdf/url and generates question, further context is matched to the RAG response for evaluating RAG.\n- Use an image processing model (e.g., CLIP, a Vision Transformer, or a CNN) to convert images into embeddings or textual descriptions.","metadata":{}},{"cell_type":"markdown","source":"# Code given (ss)","metadata":{}},{"cell_type":"code","source":"!pip install pdfplumber","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install anthropic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%\nimport re\nimport json\nimport time\nimport pdfplumber\nimport boto3\nfrom botocore.config import Config\nfrom anthropic import Anthropic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize Anthropic client\nclient = Anthropic()\n\n# Constants\nMAX_ATTEMPTS = 1\nsession_cache = {}\n\n# Configure boto3 clients with extended timeout\nmy_config = Config(\n    connect_timeout=60 * 3,\n    read_timeout=60 * 3,\n)\nbedrock = boto3.client(service_name='bedrock-runtime', region_name='eu-central-1', config=my_config)\nbedrock_service = boto3.client(service_name='bedrock', config=my_config, region_name='eu-central-1')\n\ndef ask_claude(messages, system=\"\", DEBUG=False, model='sonnet'):\n    '''\n    Send a prompt to Bedrock and return the response.\n\n    Args:\n    - messages (str or list): A single message or a list of role/message pairs.\n    - system (str): Optional. The system to send the message to.\n    - DEBUG (bool): Optional. If True, print debug information.\n    - model (str): Optional. The model to use for generating responses.\n\n    Returns:\n    - list: [raw_prompt_text, response_text] containing the original prompt and the response received.\n    '''\n    raw_prompt_text = str(messages)\n    \n    if isinstance(messages, str):\n        messages = [{\"role\": \"user\", \"content\": messages}]\n    \n    prompt_json = {\n        \"system\": system,\n        \"messages\": messages,\n        \"max_tokens\": 100000,\n        \"temperature\": 0.1,\n        \"anthropic_version\": \"\",\n        \"top_k\": 500,\n        \"stop_sequences\": [\"\\n\\nHuman:\"]\n    }\n    \n    # if DEBUG:\n    #     print(\"Sending:\\nSystem:\\n\", system, \"\\nMessages:\\n\", \"\\n\".join(messages))\n    \n    modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n    \n    # if raw_prompt_text in session_cache:\n    #     return [raw_prompt_text, session_cache[raw_prompt_text]]\n    \n    attempt = 1\n    while True:\n        try:\n            response = bedrock.invoke_model(body=json.dumps(prompt_json), modelId=modelId, accept='application/json', contentType='application/json')\n            response_body = json.loads(response['body'].read())\n            results = response_body.get(\"content\", [{}])[0].get(\"text\", \"\")\n            if DEBUG:\n                print(\"Received:\", results)\n            break\n        except Exception as e:\n            print(\"Error with calling Bedrock: \" + str(e))\n            attempt += 1\n            if attempt > MAX_ATTEMPTS:\n                print(\"Max attempts reached!\")\n                results = str(e)\n                break\n            else:\n                time.sleep(10)\n    \n    # session_cache[raw_prompt_text] = results\n    return [raw_prompt_text, results]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Section-wise chunking (ss)","metadata":{}},{"cell_type":"code","source":"# Initialize Anthropic client\nclient = Anthropic()\n\ndef count_tokens(text):\n    '''\n    Count the number of tokens in the provided text using Anthropic's client.\n\n    Args:\n    - text (str): The text for which tokens need to be counted.\n\n    Returns:\n    - int: Number of tokens in the text.\n    '''\n    return client.count_tokens(text)\n\ndef extract_all_text(pdf_path):\n    '''\n    Extract all text from a PDF document.\n\n    Args:\n    - pdf_path (str): Path to the PDF file.\n\n    Returns:\n    - dict: Dictionary where keys are page numbers and values are extracted text strings.\n    '''\n    with pdfplumber.open(pdf_path) as pdf:\n        text_dict = {}\n        for i, page in enumerate(pdf.pages, start=1):\n            if i == 1:\n                text_dict[i] = page.extract_text(layout=False, strip=True, return_chars=True)\n                \n            else:\n                text_dict[i] = '\\n'.join(line for line in page.extract_text().split('\\n') if 'Official Journal of the European Union' not in line)\n    return text_dict\n\n# Function to concatenate lines where the element before the period (.) is the same\ndef concat_lines_by_same_element(text):\n    '''\n    Concatenate lines where the element before the period (.) is the same into blocks.\n\n    Args:\n    - text (str): Text to process.\n\n    Returns:\n    - list: List of concatenated blocks.\n    '''\n    lines = text.split('\\n')\n    concatenated_blocks = []\n    current_block = \"\"\n    current_element = None\n    \n    for line in lines:\n        match = re.match(r'^(\\d+)\\.', line.strip())\n        if match:\n            element = match.group(1)\n            if current_element is None:\n                if current_block:\n                    concatenated_blocks.append(current_block.strip())\n                current_element = element\n                current_block = line\n            elif element == current_element:\n                current_block += \" \" + line\n            else:\n                concatenated_blocks.append(current_block.strip())\n                current_element = element\n                current_block = line\n        else:\n            current_block += \" \" + line\n    \n    if current_block:\n        concatenated_blocks.append(current_block.strip())\n    \n    return concatenated_blocks\n\ndef concatenate_blocks(blocks, max_tokens=2000):\n    '''\n    Concatenate text blocks ensuring the total token count for each concatenated block does not exceed the specified limit.\n\n    Args:\n    - blocks (list): List of text blocks to concatenate.\n    - max_tokens (int): Maximum number of tokens allowed per concatenated block (default is 2000).\n\n    Returns:\n    - list: List of concatenated blocks where each block's total token count is within the specified limit.\n    '''\n    concatenated_blocks = []\n    temp_block = []\n    total_value = 0\n    \n    for ele in blocks:\n        value = count_tokens(ele)\n        if total_value + value <= max_tokens:\n            temp_block.append(ele)\n            total_value += value\n        else:\n            print(f\"Total tokens for current block: {total_value}\")\n            concatenated_blocks.append(' '.join(temp_block))\n            temp_block = [ele]\n            total_value = value\n    \n    # Add the last block if it's not empty\n    if temp_block:\n        concatenated_blocks.append(' '.join(temp_block))\n    \n    return concatenated_blocks\n\n# %%\n# Example usage of extract_all_text function\npdf_path = path1\nall_text = extract_all_text(pdf_path)\n\n# %%\nfull_text = '\\n'.join(all_text.values())\n# Encode some misc unicode characters\nfull_text = full_text.encode('utf-8').decode()\n# Example usage of concat_lines_by_same_element function\nblocks = concat_lines_by_same_element(full_text)  # Assuming all_text is a dictionary with page numbers\nprint(blocks)\nblocks_1 = concatenate_blocks(blocks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(blocks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"blocks_1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(blocks_1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%\nlong_prompt_template = \"\"\"Consider the following portion of the document.\n<document>\n{{document}}\n</document>\n\nPlease explain all the points of test condition mentioned in test procedure in details?\nExtract all the contents related to query and then summarize the extracted contents into one response.\nReturn the answer in <response> tag, and if could not find the answer return empty <response> tag,\nand also return the confidence as percentage of the answers in <confidence> tag. \n\"\"\"\n\nlong_prompt = long_prompt_template.replace(\"{{document}}\",full_text)\nlong_responce = ask_claude(long_prompt, model=\"sonnet\")[1]\nprint(long_responce)\n# %%\nanswer_output = []\nfor ele in blocks_1:\n    long_prompt = long_prompt_template.replace(\"{{document}}\",ele)\n    long_responce = ask_claude(long_prompt, model=\"sonnet\")[1]\n    answer_output.extend([long_responce])\n    \nanswer_all = \"\".join(answer_output)\n# %%\nsummarize_template = \"\"\"Consider the following responses. It has fetched for a single query from different sections of\nthe document.\n\n<response>\n{{response}}\n</response>\n\nSummarize these responses into one response.\nReturn the answer in <response> tag, and if could not find the answer return \"Could not find the answer\" in <response> tag,\nand also return the confidence as percentage of the answers in <confidence> tag. \n\"\"\"\n\noutput = ask_claude(summarize_template.replace(\"{{response}}\",answer_all), model=\"sonnet\")[1]\n\n\n# %%\nprint(output)\n# %%","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Section-wise chunking (smb)","metadata":{}},{"cell_type":"code","source":"import pdfplumber\nimport re\n\ndef read_pdf_pagewise(file_path):\n    \n    section_pattern = re.compile(r'(\\d [A-Z]|Annex [A-Z])')\n\n    sections = {}\n    with pdfplumber.open(file_path) as pdf:\n        num_pages = len(pdf.pages)\n        print(f'Total pages: {num_pages}')\n\n        current_section = None\n        for page_num in range(2, num_pages):\n            page = pdf.pages[page_num]\n            page_text = page.extract_text()\n\n            if page_text:\n                lines = page_text.split('\\n')\n                if len(lines) > 3:  \n                    lines = lines[1:-2]\n                else:\n                    lines = [] \n                \n                for line in lines:\n                    match = section_pattern.match(line)\n                    if match:\n                        current_section = match.group(1)\n                        if current_section not in sections:\n                            sections[current_section] = []\n                    \n                    if current_section:\n                        sections[current_section].append(line)\n    return sections\n#     for section, content in sections.items():\n#         print(f\"--- {section} ---\")\n#         print('\\n'.join(content))\n#         print(\"\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pdfplumber\nimport re\n\ndef read_pdf_pagewise(file_path):\n    \n    section_pattern = re.compile(r'(\\d [A-Z]|Annex [A-Z])')\n\n    sections = {}\n    with pdfplumber.open(file_path) as pdf:\n        num_pages = len(pdf.pages)\n        print(f'Total pages: {num_pages}')\n\n        current_section = None\n        for page_num in range(2, num_pages):\n            page = pdf.pages[page_num]\n            page_text = page.extract_text()\n\n            if page_text:\n                lines = page_text.split('\\n')\n                if len(lines) > 3:  \n                    lines = lines[1:-2]\n                else:\n                    lines = [] \n                \n                for line in lines:\n                    match = section_pattern.match(line)\n                    if match:\n                        current_section = match.group(1)\n                        if current_section not in sections:\n                            sections[current_section] = []\n                    \n                    if current_section:\n                        sections[current_section].append(line)\n    for section, content in sections.items():\n        print(f\"--- {section} ---\")\n        print('\\n'.join(content))\n        print(\"\\n\\n\")\n        \nread_pdf_pagewise(path4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}