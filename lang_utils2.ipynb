{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27154920-ab0c-48ed-885f-3d3988137d4b",
   "metadata": {},
   "source": [
    "Reference github repo: https://github.com/felipesanma/pdf-comparison/tree/main?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ae35c2-9c97-4c5b-8fd0-315019289957",
   "metadata": {},
   "source": [
    "# Replacong with open source models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3dc72c-a0b7-4e72-b992-948d62f62611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirments.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirments.txt\n",
    "\n",
    "PyPDF2 \n",
    "sentence-transformers \n",
    "faiss-cpu \n",
    "langchain \n",
    "streamlit \n",
    "langchain_community "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c7ead9-5800-41d2-a5f0-0d399ec1cac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VISHAL MAURYA\\Desktop\\Document Comparision\\lang_venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFaceHub\n",
    "import streamlit as st\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a51f85f-a1a8-4301-94e7-583b82e3fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_splitter(pdf_file):\n",
    "    pdf_reader = PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, chunk_overlap=0, length_function=len\n",
    "    )\n",
    "    result = text_splitter.split_text(text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d954ae-c552-4f10-aa78-503e1b0032c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = get_text_splitter(\"data/GB 25991-2010 Automotive Headlamps with LED Light Sources andor LED Modules (1) (2).pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df4b215a-8bff-4aa8-9751-742e58da2e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_retrievals(pdf_file_list: list):\n",
    "    qa_retrievals = []\n",
    "    embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    for pdf in pdf_file_list:\n",
    "        texts = get_text_splitter(pdf)\n",
    "        docsearch = FAISS.from_texts(texts = texts, embedding = embedding_model)\n",
    "        # st.info(f\"Saving {pdf} to vector DB\")\n",
    "\n",
    "        # Using HuggingFaceHub for the LLaMA model\n",
    "        # llm = HuggingFaceHub(model=\"meta-llama/LLaMA-7b\", model_kwargs={\"temperature\": 0.1})\n",
    "        llm = HuggingFaceHub(repo_id=\"facebook/opt-1.3b\", model_kwargs={\"temperature\": 0.1})\n",
    "        qa_tmp = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=docsearch.as_retriever(\n",
    "                search_type=\"similarity\", search_kwargs={\"k\": 2}\n",
    "            ),\n",
    "            return_source_documents=True,\n",
    "        )\n",
    "        qa_retrievals.append(qa_tmp)\n",
    "\n",
    "    return qa_retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f84ab25-a90d-41f2-8739-a58ed3456b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VISHAL MAURYA\\Desktop\\Document Comparision\\lang_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features type =  <class 'dict'>\n",
      "features length =  3\n",
      "features {'input_ids': tensor([[  101,  2795,  1017,  ...,  4618, 23758,   102],\n",
      "        [  101,  5852,  2005,  ...,     0,     0,     0],\n",
      "        [  101,  1020,  1012,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  4460,  1998,  ...,     0,     0,     0],\n",
      "        [  101,  1020,  1012,  ...,  3048, 23190,   102],\n",
      "        [  101,  2031,  3478,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "features type =  <class 'dict'>\n",
      "features length =  3\n",
      "features {'input_ids': tensor([[  101,  6705,  1997,  ...,     0,     0,     0],\n",
      "        [  101,  5491,  5393,  ...,     0,     0,     0],\n",
      "        [  101,  1039,  1012,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1037,  1041,  ...,  2125,  1025,   102],\n",
      "        [  101, 11336,  1998,  ...,     0,     0,     0],\n",
      "        [  101,  1039,  1012,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n",
      "C:\\Users\\VISHAL MAURYA\\Desktop\\Document Comparision\\lang_venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceHub\n__root__\n  Did not find huggingfacehub_api_token, please add an environment variable `HUGGINGFACEHUB_API_TOKEN` which contains it, or pass `huggingfacehub_api_token` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_retrievals \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_qa_retrievals\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/GB 25991-2010 Automotive Headlamps with LED Light Sources andor LED Modules (1) (2).pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mcreate_qa_retrievals\u001b[1;34m(pdf_file_list)\u001b[0m\n\u001b[0;32m      6\u001b[0m docsearch \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_texts(texts \u001b[38;5;241m=\u001b[39m texts, embedding \u001b[38;5;241m=\u001b[39m embedding_model)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# st.info(f\"Saving {pdf} to vector DB\")\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Using HuggingFaceHub for the LLaMA model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# llm = HuggingFaceHub(model=\"meta-llama/LLaMA-7b\", model_kwargs={\"temperature\": 0.1})\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceHub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/opt-1.3b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m qa_tmp \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[0;32m     13\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m     14\u001b[0m     chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     return_source_documents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m qa_retrievals\u001b[38;5;241m.\u001b[39mappend(qa_tmp)\n",
      "File \u001b[1;32m~\\Desktop\\Document Comparision\\lang_venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:183\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     emit_warning()\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Document Comparision\\lang_venv\\Lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for HuggingFaceHub\n__root__\n  Did not find huggingfacehub_api_token, please add an environment variable `HUGGINGFACEHUB_API_TOKEN` which contains it, or pass `huggingfacehub_api_token` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "qa_retrievals = create_qa_retrievals([\"data/GB 25991-2010 Automotive Headlamps with LED Light Sources andor LED Modules (1) (2).pdf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecaa325-deb5-4957-9a4e-594ebf4f55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(qa_retrievals), len(qa_retrievals), type(qa_retrievals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d2ba4-63a9-4440-a2e1-87374aac82ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004492ee-2e1f-4d4b-b898-d9e1fbba0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_to_all_pdfs_sources(query: str, qa_retrievals):\n",
    "    responses = []\n",
    "    progress_text = f\"Asking '{query}' to all PDFs\"\n",
    "    total_retrievals = len(qa_retrievals)\n",
    "    my_bar = st.progress(0, text=progress_text)\n",
    "    for count, qa in enumerate(qa_retrievals):\n",
    "        result = qa({\"query\": query})\n",
    "        tmp_obj = {\n",
    "            \"query\": query,\n",
    "            \"response\": result[\"result\"],\n",
    "            \"source_document\": result[\"source_documents\"][0]\n",
    "            .metadata[\"source\"]\n",
    "            .split(\"-\")[1],\n",
    "        }\n",
    "        responses.append(tmp_obj)\n",
    "        percent_complete = (count + 1) * 100 / total_retrievals\n",
    "        my_bar.progress(int(percent_complete), text=progress_text)\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984424d2-26e8-416d-9e41-bf5f92ddbaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are regulations on color rendering?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7976c6b3-3ca9-4ce1-babf-64aa401e1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_to_all_pdfs_sources(query, qa_retrievals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00201312-290e-4ecf-8c32-72ed1ea822c2",
   "metadata": {},
   "source": [
    "#### Function forward in module ~\\Desktop\\Document Comparision\\lang_venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py expects a dictionary but getting an str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab9147-846f-4113-9ce4-47de05f94293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5707c2-e62e-470a-a1d3-93c9b62bb522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8bed11-5e8d-4209-9a40-2e9d7df9a158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c0afef-20b5-49d7-abde-1cd805d530a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e946160-e3c0-4e72-98e9-a57db158b4e0",
   "metadata": {},
   "source": [
    "## Repo code with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8953daa-54dc-4701-8de0-5b85a640d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import streamlit as st\n",
    "# from dotenv import load_dotenv\n",
    "# from langchain import OpenAI\n",
    "# from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "# from langchain.document_loaders import UnstructuredPDFLoader\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.vectorstores import Chroma\n",
    "# from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27caf3-222c-4e2b-8c8d-a67b94732582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_text_splitter(pdf_file):\n",
    "#     pdf_reader = PdfReader(pdf_file)\n",
    "#     text = \"\"\n",
    "#     # st.info(f\"Extracting text from PDF {pdf_file.name}\")\n",
    "#     for page in pdf_reader.pages:\n",
    "#         text += page.extract_text()\n",
    "#     # st.info(f\"Getting Chunks from {pdf_file.name}\")\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=1000, chunk_overlap=0, length_function=len\n",
    "#     )\n",
    "#     result = text_splitter.split_text(text)\n",
    "#     return result\n",
    "\n",
    "\n",
    "# def create_qa_retrievals(pdf_file_list: list, OPENAI_API_KEY):\n",
    "\n",
    "#     qa_retrievals = []\n",
    "#     for pdf in pdf_file_list:\n",
    "#         # st.info(f\"Processing {pdf.name}\")\n",
    "#         texts = get_text_splitter(pdf)\n",
    "#         # st.info(f\"Converting PDF {pdf.name} to embedding\")\n",
    "#         docsearch = Chroma.from_texts(\n",
    "#             texts,\n",
    "#             OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY),\n",
    "#             metadatas=[{\"source\": f\"{i}-{pdf.name}\"} for i in range(len(texts))],\n",
    "#         )\n",
    "#         st.info(f\"Saving {pdf.name} to vector DB\")\n",
    "#         qa_tmp = RetrievalQA.from_chain_type(\n",
    "#             llm=OpenAI(openai_api_key=OPENAI_API_KEY),\n",
    "#             chain_type=\"stuff\",\n",
    "#             retriever=docsearch.as_retriever(\n",
    "#                 search_type=\"similarity\", search_kwargs={\"k\": 2}\n",
    "#             ),\n",
    "#             return_source_documents=True,\n",
    "#         )\n",
    "#         qa_retrievals.append(qa_tmp)\n",
    "\n",
    "#     return qa_retrievals\n",
    "\n",
    "\n",
    "# def ask_to_all_pdfs_sources(query: str, qa_retrievals):\n",
    "#     responses = []\n",
    "#     progress_text = f\"Asking '{query}' to all PDF's\"\n",
    "#     total_retrievals = len(qa_retrievals)\n",
    "#     my_bar = st.progress(0, text=progress_text)\n",
    "#     for count, qa in enumerate(qa_retrievals):\n",
    "#         result = qa({\"query\": query})\n",
    "#         tmp_obj = {\n",
    "#             \"query\": query,\n",
    "#             \"response\": result[\"result\"],\n",
    "#             \"source_document\": result[\"source_documents\"][0]\n",
    "#             .metadata[\"source\"]\n",
    "#             .split(\"-\")[1],\n",
    "#         }\n",
    "#         responses.append(tmp_obj)\n",
    "#         percent_complete = (count + 1) * 100 / total_retrievals\n",
    "#         my_bar.progress(int(percent_complete), text=progress_text)\n",
    "\n",
    "#     return responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lang_venv)",
   "language": "python",
   "name": "lang_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
